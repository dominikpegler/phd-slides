<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Machine Learning for Human-Centered Solutions</title>
<meta name="author" content="Dominik Pegler"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/white.css" id="theme"/>

<link rel="stylesheet" href="css/custom.css"/>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<div><h1>Machine Learning for Human-Centered Solutions</h1><h3 style="color:#6b6b6b">Interpretability, Emotion Recognition, and Therapeutic Innovation<h3><p style="text-transform:none;color:black;font-weight:normal">Dominik Pegler<p></div>
</section>
<section>
<section id="slide-org3d22923">
<h2 id="org3d22923">Overview</h2>
<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;">Enhancing cognitive and affective processes in human-AI interaction through machine learning</div>

<div style="display:flex;flex-direction:column;font-size:2.0rem">

<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:8px;margin-bottom:4px;padding:8px">

<div style="color:#7e7e7e;font-weight:bold;font-size:1.4rem;margin-bottom:4px">Cognitive Domain: Problem Solving</div>

<div style="display:flex; flex-direction:row; margin-top:1rem">

<div style="width:15%;">

<div id="org4130348" class="figure">
<p class="org-p-fig"><img src="./img/p1.png" alt="p1.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">#1 – Interpretability</div>
<div style="font-size:1.6rem;text-align:left">Enhancing joint human-machine problem solving by optimizing the machine for interpretability</div>

</div>
</div>

</div>

<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:4px;margin-bottom:0px;padding:8px">

<div style="color:#7e7e7e;font-size:1.4rem;font-weight:bold;margin-bottom:4px">Affective Domain: Exposure Therapy</div>

<div style="display:flex; flex-direction:row; margin-top:1rem">

<div style="width:15%;">

<div id="orgb90d062" class="figure">
<p class="org-p-fig"><img src="./img/p2.png" alt="p2.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">#2 – Fear Prediction in Images</div>


<div style="font-size:1.6rem;text-align:left">Advancing computer-aided Exposure Therapy by automatically evaluating fear-related stimuli through artificial neural networks.</div>

</div>
</div>

<div style="display:flex; flex-direction:row; margin-top:1rem">

<div style="width:15%;">

<div id="org8d7f462" class="figure">
<p class="org-p-fig"><img src="./img/p3.png" alt="p3.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">#3 – Optimizing Stimulus Sequences</div>

<div style="font-size:1.6rem;text-align:left"> Advancing computer-aided Exposure Therapy by optimizing therapy protocols using reinforcement learning (RL).</div>

</div>
</div>


</div>

</div>
</section>
</section>
<section>
<section id="slide-org7b3a4d2">
<h2 id="org7b3a4d2">#1</h2>
<p>
<b>INTERPRETABILITY OF MACHINE-GENERATED SOLUTIONS TO COMBINATORIAL DESIGN PROBLEMS</b>
</p>


<div id="orgdff2c2c" class="figure">
<p class="org-p-fig"><img src="./img/p1.png" alt="p1.png" height="490px" />
</p>
</div>
</section>
<section id="slide-orgf995285">
<h3 id="orgf995285">Problem Setting</h3>
<aside class="notes">
<p>
Today, I am going to present you a new study on the topic of human-machine colaboration.
</p>

</aside>

<div style="display: flex; flex-direction: column">
<div style="display: flex; flex-direction: row;">
<div style="width:80%">
<p class="fragment (t)">
<b>Machine Problem-Solving</b>
</p>
<ul>
<li class="fragment">Increasingly taking over human domains</li>
<li class="fragment">AI getting more complex &rarr; black boxes &rarr; lack of trust</li>
<li class="fragment">Trust issues not new (Classical AI in 1950s)</li>

</ul>
</div>
<div style="width:20%">
<p>
 </p><div class="imgcontainer"><div about="./img/./dantzig_c.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./dantzig_c.jpg" alt="Goerge Dantzig" style="max-height:176px" /></p><p property="schema:caption">George Dantzig (1914-2005), father of linear programming</p><p style="width:'1881'px;max-width:176px">&ldquo;<span property="dcterms:title">George Dantzig.</span>&rdquo;  under <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a>; from <a rel="dcterms:source" href="https://malevus.com/george-dantzig/">malevus.com</a></p></div></div><p>
</p>
</div>
</div>

<div style="display: flex; flex-direction: row;">
<div style="width:80%">
<p class="fragment (t)">
<b>Evaluating Human Interpretability</b>
</p>
<ul>
<li class="fragment">Human-in-the-loop approach to evaluate interpretability</li>
<li class="fragment">Understanding how a machine makes a decision</li>
<li class="fragment">Critical for trust and collaboration with machines</li>

</ul>
</div>
<div style="width:20%">
<p>
 </p><div class="imgcontainer"><div about="./svg/human_loop.svg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./svg/human_loop.svg" alt="Human-in-the-loop" style="max-height:176px" /></p><p property="schema:caption">Human-in-the-loop: Enhanced algorithms through continuous human input.</p><p style="width:'540'px;max-width:176px">&ldquo;<span property="dcterms:title">Human-in-the-loop</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://docs.google.com" property="cc:attributionName">Dominik Pegler</a> under <a rel="license" href="https://creativecommons.org/licenses/by-sa/2.5/">CC BY-SA 2.5 Generic</a>; from <a rel="dcterms:source" href="https://docs.google.com">Created using Google Docs</a></p></div></div><p>
</p>
</div>
</div>
</div>
</section>
<section id="slide-org41a52c0">
<h3 id="org41a52c0">Combinatorial Design Problems</h3>
<div style="display:flex;flex-direction:column;height: 100%">
<div style="display:flex; flex-direction:row;justify-content:space-between;height:40%;">

<p>
 </p><div class="imgcontainer"><div about="./svg/./knapsack.svg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./svg/./knapsack.svg" alt="Knapsack" style="max-height:176px" /></p><p property="schema:caption">Knapsack Problem</p><p style="width:'421'px;max-width:176px">&ldquo;<span property="dcterms:title">Knapsack</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://commons.wikimedia.org/wiki/File:Knapsack.svg#filehistory" property="cc:attributionName">Dake~commonswiki and Keenan Pepper</a> under <a rel="license" href="https://creativecommons.org/licenses/by-sa/2.5/">CC BY-SA 2.5 Generic</a>; from <a rel="dcterms:source" href="https://commons.wikimedia.org/wiki/File:Knapsack.svg">Wikimedia Commons</a></p></div></div><p>
</p>
<p>
 </p><div class="imgcontainer"><div about="./svg/./minimum_spanning_tree.svg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./svg/./minimum_spanning_tree.svg" alt="Traveling Salesmans" style="max-height:176px" /></p><p property="schema:caption">Minimum Spanning Tree</p><p style="width:'242'px;max-width:176px">&ldquo;<span property="dcterms:title">Traveling Salesmans</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://commons.wikimedia.org/wiki/File:Minimum_spanning_tree.svg#filehistory" property="cc:attributionName">Dcoetzee</a> under <a rel="license" href="https://en.wikipedia.org/wiki/Public_domain">Public domain</a>; from <a rel="dcterms:source" href="https://commons.wikimedia.org/wiki/File:Minimum_spanning_tree.svg">Wikimedia Commons</a></p></div></div><p>
</p>
<p>
 </p><div class="imgcontainer"><div about="./svg/./traveling_salesman.svg.png" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./svg/./traveling_salesman.svg.png" alt="Traveling Salesmans" style="max-height:176px" /></p><p property="schema:caption">Traveling Salesman Problem</p><p style="width:'477'px;max-width:176px">&ldquo;<span property="dcterms:title">Traveling Salesmans</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://commons.wikimedia.org/wiki/File:GLPK_solution_of_a_travelling_salesman_problem.svg#filehistory" property="cc:attributionName">Xypron</a> under <a rel="license" href="https://en.wikipedia.org/wiki/Public_domain">Public domain</a>; from <a rel="dcterms:source" href="https://commons.wikimedia.org/wiki/File:GLPK_solution_of_a_travelling_salesman_problem.svg">Wikimedia Commons</a></p></div></div><p>
</p>
</div>
<div style="display:flex;flex-direction:column;max-height:60%">
<ul>
<li class="fragment">Many real-world scenarios (logistics, etc.)</li>
<li class="fragment">Can be solved by machines optimally (e.g., with Linear Programming)</li>
<li class="fragment">Can be solved by humans (if problem is small enough)</li>

</ul>
<p class="fragment (t)">
&rarr; <b><b>Good setting for human-machine collaboration</b></b>
</p>
</div>
</div>
</section>
<section id="slide-orgdecfb0c">
<h3 id="orgdecfb0c">Bin-Packing Problem</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:70%;">
<ul>
<li>Abstract representation of real-world scenarios (e.g., scheduling)</li>
<li>Pack items into boxes</li>
<li>Goal: Fill the boxes as much as possible</li>
<li>Constraint: You cannot overfill the boxes</li>

</ul>
</div>
<div style="display:flex;flex-direction:column;max-width:30%">

<div id="orgcac1724" class="figure">
<p class="org-p-fig"><img src="./img/binpacking.gif" alt="binpacking.gif" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 1: </span>A human performing the bin packing task.</p>
</div>
</div>
</div>
</section>
<section id="slide-org69b98de">
<h3 id="org69b98de">Optimal Solutions</h3>

<div id="org8b62c93" class="figure">
<p class="org-p-fig"><img src="./img/optimalsolutions.gif" alt="optimalsolutions.gif" height="600px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 2: </span>The machine (<a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>) providing possible optimal solutions.</p>
</div>
</section>
<section id="slide-org23ef066">
<h3 id="org23ef066">Question</h3>
<p>
<b>"What makes a solution interpretable?"</b>
</p>
</section>
<section id="slide-org899c9c4">
<h3 id="org899c9c4">H1: Heuristic</h3>
<ul>
<li>Humans use (greedy) heuristics &rarr; greedy solution</li>
<li>Similarity to greedy solution is measured by graph edit distance(<a href="#citeproc_bib_item_6">Sanfeliu &#38; Fu, 1983</a>)</li>

</ul>


<div id="orgcc199d9" class="figure">
<p class="org-p-fig"><img src="./svg/heuristic.svg" alt="heuristic.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if similar to greedy solution</b>
</p>
</section>
<section id="slide-orgdbefb2f">
<h3 id="orgdbefb2f">H2: Simplicity</h3>
<ul>
<li>Bins can look more or less simple/complex</li>
<li>Formalized as log-probability that a mixture model (2 dirichlet, 1 geometric distribution) returns for each bin composition</li>

</ul>


<div id="org2f2a401" class="figure">
<p class="org-p-fig"><img src="./svg/composition.svg" alt="composition.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if simple</b>
</p>
</section>
<section id="slide-org03184a4">
<h3 id="org03184a4">H3: Representation</h3>
<ul>
<li>Items and boxes can be sorted by size or at random</li>
<li>Formalized as rank correlation between the actual order and the sorted order</li>

</ul>


<div id="org0bd1251" class="figure">
<p class="org-p-fig"><img src="./svg/order.svg" alt="order.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if sorted</b>
</p>
</section>
<section id="slide-org6f574ef" data-auto-animate>
<h3 id="org6f574ef">Online-Experiment</h3>
<img src="svg/experiment_1.svg" alt="experiment overview"/>
<p>
<i>N</i> = 73 participants (pilot)
</p>
</section>
<section id="slide-org1fb1943" data-auto-animate>
<h3 id="org1fb1943">Online-Experiment</h3>
<img src="svg/experiment_2.svg" alt="experiment overview"/>
<p>
<i>N</i> = 73 participants (pilot)
</p>
</section>
<section id="slide-org5dc8188">
<h3 id="org5dc8188">Pilot Results</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:60%;">


<div id="org49f0a92" class="figure">
<p class="org-p-fig"><img src="./img/results_choice_fixed_effects.png" alt="results_choice_fixed_effects.png" height="100%" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 3: </span>Fixed Effects Estimates of Predictor Variables on Choice in Multilevel Analysis. The plot displays the estimated fixed effects (with 95% confidence intervals) for the three predictors. The effects are adjusted for random effects at the group level, highlighting the marginal impact of each predictor on the outcome variable 'choice'.</p>
</div>
</div>

<div style="display:flex; flex-direction:column;justify-content:center;width:40%;">
<ul>
<li><b>All three</b> predictors relevant for people's choices</li>
<li><b>Order</b> and <b>Heuristic</b> most influental</li>
<li><b>Moderate effect</b>: <i>R²</i>=0.17 (<a href="#citeproc_bib_item_1">Cohen, 1988</a>)</li>

</ul>
<div style="font-size:1.5rem;">
<p>
Other findings:
</p>
<ul>
<li>Considerable participant variability in all predictors</li>
<li>Self-reported problem-solving skills ("PSI") and solving performance do not moderate choice</li>

</ul>
</div>
</div>
</div>
</section>
<section id="slide-orgb200653">
<h3 id="orgb200653">Pilot Results Eye-tracking</h3>
<div style="display:flex;flex-direction:column;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;">
<ul>
<li><b>Gaze dwell times</b> predictive of choice (<i>r</i> = 0.20)</li>
<li>&Delta; <i>t</i> = <i>t<sub>right</sub><sub>box</sub></i> - <i>t<sub>left</sub><sub>box</sub></i></li>
<li>Webcam-Eye-Tracking using <a href="https://webgazer.cs.brown.edu/">WebGazer.js</a></li>

</ul>

</div>
<div style="display:flex; flex-direction:row;justify-content:center;">



<div id="org64b39da" class="figure">
<p class="org-p-fig"><img src="./img/eye_example.png" alt="eye_example.png" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 4: </span>Heatmap indicating gaze dwell times during an evaluation trial.</p>
</div>


<div id="orga1d5eb5" class="figure">
<p class="org-p-fig"><img src="./img/eye_correlation.png" alt="eye_correlation.png" height="450px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 5: </span>Correlation (with 95% confidence interval) of gaze dwell time (right versus left) with the outcome variable 'choice', highlighting the relationship between where participants direct their gaze and their choices.</p>
</div>

</div>
</div>
</section>
<section id="slide-orge784bf6">
<h3 id="orge784bf6">Limitations</h3>
<ul>
<li>Did we measure <b>interpretability</b>?</li>
<li>Focus on <b>small problems</b> (diversity of solutions limited)</li>
<li>Only tested for optimal solutions, <b>no suboptimal solutions</b></li>
<li>Eye-Tracking via webcam very noisy</li>

</ul>

<p>
<b>Possible next steps</b>
</p>

<div id="orgd8e1bc8" class="figure">
<p class="org-p-fig"><img src="./svg/next_steps.svg" alt="next_steps.svg" class="org-svg" width="100%" margin-top="0px" margin-bottom="0px" />
</p>
</div>
</section>
<section id="slide-org6bd09e9">
<h3 id="org6bd09e9">Takeaways</h3>
<ul>
<li>Humans seem to use <b>solving heuristics during evaluation</b>​</li>
<li>Adequate <b>visual representation</b> is requirement​</li>
<li>All factors may play a bigger role in <b>larger problems</b>​</li>
<li><b>Validation</b> required</li>

</ul>

<div style="font-size:0.8rem;margin-top:5rem">
<p>
This project is supported by <a href="https://www.ffg.at">www.ffg.at</a>
</p>
</div>

<div id="orga240ab0" class="figure">
<p class="org-p-fig"><img src="./svg/ffg_logo_en.svg" alt="ffg_logo_en.svg" class="org-svg" height="40px" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-org6d1a998">
<h2 id="org6d1a998">#2</h2>
<p>
<b>LEARNING AND LOCALIZING FEAR WITH COMPUTER VISION MODELS</b>
</p>


<div id="org36a383e" class="figure">
<p class="org-p-fig"><img src="./img/p2_alt.png" alt="p2_alt.png" height="400px" />
</p>
</div>
</section>
<section id="slide-orgc8713c8" data-auto-animate>
<h3 id="orgc8713c8">Problem Setting</h3>
<div style="float:left;width:78%;">
<p>
<b>Background: "Affective Computing" (<a href="#citeproc_bib_item_5">Picard, 1997</a>)</b>
</p>
<ul>
<li>Technology that relates to, arises from, or influences emotions</li>
<li>For effective and natural human-computer interactions, computers must not only recognize but also respond to human emotions</li>

</ul>

<p>
<b>Our study: Phobia Research</b>
</p>
<ul>
<li>Aim: Advance computer-aided exposure therapy</li>
<li>Focus: Spider phobia</li>

</ul>
</div>

<div style="float:right;width:22%;">
<p>
 </p><div class="imgcontainer"><div about="./img/./affective_computing.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./affective_computing.jpg" alt="Affective Computing." style="max-height:45rh" /></p><p property="schema:caption">Affective Computing (1997) by Rosalind Picard.</p><p style="width:'1956'px;max-width:45rh">&ldquo;<span property="dcterms:title">Affective Computing (1997).</span>&rdquo;  under All rights reserved or Unknown licensing information; from <a rel="dcterms:source" href="https://cdn2.penguin.com.au/covers/original/9780262661157.jpg">https://www.penguin.com.au/</a></p></div></div><p>
</p>
</div>
</section>
<section id="slide-orgee0f5c7" data-auto-animate>
<h3 id="orgee0f5c7">Problem Setting</h3>
<div style="display:flex;flex-direction:column;height: 100%">
<div style="display:flex; flex-direction:row;justify-content:space-evenly;height:40%;">



<div id="org6bf4746" class="figure">
<p class="org-p-fig"><img src="./img/example_stimuli_large.png" alt="example_stimuli_large.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 6: </span>The stimulus set.</p>
</div>


<div id="org8b058f7" class="figure">
<p class="org-p-fig"><img src="./img/fear_ratings.png" alt="fear_ratings.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 7: </span>Rating the fear level of each image.</p>
</div>


<div id="orgabc700b" class="figure">
<p class="org-p-fig"><img src="./img/example_stimuli_rated.png" alt="example_stimuli_rated.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 8: </span>Example rated images.</p>
</div>

</div>
<div style="display:flex;flex-direction:column;max-height:60%">

<ul>
<li class="fragment"><b>Spider images</b> for exposure therapy
<ul>
<li>Usage requires information, e.g., how much fear they provoke</li>
<li>Fear ratings for 313 spider images collected</li>

</ul></li>

</ul>
<ul>
<li class="fragment"><b>Problem:</b> Set is limited to 313
<ul>
<li>Constantly collecting new fear ratings for each new stimulus not
feasible</li>

</ul></li>

</ul>
</section>
<section id="slide-orgd471bac">
<h3 id="orgd471bac">Deep Neural Networks</h3>
<ul>
<li>Solution: Use deep neural networks to create larger stimulus sets (<a href="#citeproc_bib_item_4">LeCun et al., 2015</a>)</li>
<li>Pre-trained on large datasets (<a href="#citeproc_bib_item_2">Deng et al., 2009</a>)</li>

</ul>
<p>
 </p><div class="imgcontainer"><div about="./img/./cnn_architecture.png" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./cnn_architecture.png" alt="CNN" style="max-height:320px" /></p><p property="schema:caption">Architecture of a convolutional neural network (CNN)</p><p style="width:'530'px;max-width:320px">&ldquo;<span property="dcterms:title">CNN architecture.</span>&rdquo;  under <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a>; from <a rel="dcterms:source" href="https://developersbreach.com/convolution-neural-network-deep-learning">developersbreach.com</a></p></div></div><p>
</p>
<ul>
<li>Transfer Learning (<a href="#citeproc_bib_item_9">Yosinski et al., 2014</a>): Fine-tune on own data (313 images with fear ratings) &rarr; fear rating for any new image</li>

</ul>
</section>
<section id="slide-org2819eb5" data-auto-animate>
<h3 id="org2819eb5">Research questions</h3>
<div style="border-radius:12px;background-color:lightblue;">
<p class="fragment (t)">
Q1: Can a computer vision model built for object recognition learn a <b>latent construct</b> such as fear?
</p>
<div style="font-size:1.9rem;">
<p class="fragment (t)">
Q1-1: How much <b>data</b> do we need?
</p>
<p class="fragment (t)">
Q1-2: What <b>erros</b> will it make?
</p>
</div>
</div>

<div style="border-radius:12px;background-color:lightblue;margin-top:3rem">
<p class="fragment (t)">
Q2: <b>How</b> does the model arrive at its judgments and how do they differ from <b>human judgments</b>?
</p>
</div>
</section>
<section id="slide-org7ba90b7" data-auto-animate>
<h3 id="org7ba90b7">Methodology</h3>
<div style="font-size:1.8rem;border-radius:12px;background-color:lightblue;">
<p>
Q1: Can a computer vision model built for object recognition learn a <b>latent construct</b> such as fear?
</p>
</div>

<ol>
<li class="fragment">Find suitable deep learning <b>architecture</b> (<a href="https://timm.fast.ai">timm.fast.ai</a>)</li>
<li class="fragment">Write <b>training</b> pipeline + train model</li>
<li class="fragment"><b>Learning curve analysis</b>: Train multiple times with different amounts of data</li>
<li class="fragment"><b>Error analysis</b>: Which images are difficult to predict</li>

</ol>

<div style="font-size:1.8rem;border-radius:12px;background-color:lightblue;">
<p>
Q2: <b>How</b> does the model arrive at its judgments and how do they differ from <b>human judgments</b>?
</p>
</div>

<ol>
<li class="fragment"><b>Explain predictions</b>: Highlight fear-relevant regions in each image using Gradient-weighted Class Activation Mapping (Grad-CAM; <a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)</li>
<li class="fragment"><b>Alignment analysis</b>: How do the model's judgments differ from human judgments (uncertainty, fear-relevant regions)</li>

</ol>
</section>
<section id="slide-org8051e66" data-auto-animate>
<h3 id="org8051e66">Preliminary Results</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:60%;">

<div id="org9e382b1" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_results.svg" alt="cnn_results.svg" class="org-svg" height="600px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 9: </span>Predictive performance of the CNN model for each image.</p>
</div>
</div>

<div style="display:flex; flex-direction:column;justify-content:flex-start;width:40%;">
<ul>
<li>Promising predictive accuracy</li>
<li>Model: ResNet50 (<a href="#citeproc_bib_item_3">He et al., 2015</a>)</li>
<li>Training is possible on standard PC hardware (hours–days)</li>

</ul>
</div>
</div>
</section>
<section id="slide-org18edba4" data-auto-animate>
<h3 id="org18edba4">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orgfbd2df8" class="figure">
<p class="org-p-fig"><img src="./img/Sp_046_gradcam.png" alt="Sp_046_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org0b3d3a3" data-auto-animate>
<h3 id="org0b3d3a3">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="org347c000" class="figure">
<p class="org-p-fig"><img src="./img/Sp_111_gradcam.png" alt="Sp_111_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org4faf34b" data-auto-animate>
<h3 id="org4faf34b">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orge7bb3dc" class="figure">
<p class="org-p-fig"><img src="./img/Sp_012_gradcam.png" alt="Sp_012_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org6cc8052" data-auto-animate>
<h3 id="org6cc8052">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orgd047329" class="figure">
<p class="org-p-fig"><img src="./img/Sp_092_gradcam.png" alt="Sp_092_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org0d047f9" data-auto-animate>
<h3 id="org0d047f9">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orge43dd64" class="figure">
<p class="org-p-fig"><img src="./img/Sp_283_gradcam.png" alt="Sp_283_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org3b4333a" data-auto-animate>
<h3 id="org3b4333a">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orgffcc203" class="figure">
<p class="org-p-fig"><img src="./img/Sp_098_gradcam.png" alt="Sp_098_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org3131b85" data-auto-animate>
<h3 id="org3131b85">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orgecbe435" class="figure">
<p class="org-p-fig"><img src="./img/Sp_078_gradcam.png" alt="Sp_078_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org5ceff2a" data-auto-animate>
<h3 id="org5ceff2a">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="org0642d0b" class="figure">
<p class="org-p-fig"><img src="./img/Sp_073_gradcam.png" alt="Sp_073_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org022de6c" data-auto-animate>
<h3 id="org022de6c">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="orgdf9098d" class="figure">
<p class="org-p-fig"><img src="./img/Sp_285_gradcam.png" alt="Sp_285_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-orge874077" data-auto-animate>
<h3 id="orge874077">Preliminary Results</h3>
<ul>
<li><p>
Explanations with Grad-CAM (<a href="#citeproc_bib_item_7">Selvaraju et al., 2020</a>)
</p>


<div id="org8a58250" class="figure">
<p class="org-p-fig"><img src="./img/Sp_085_gradcam.png" alt="Sp_085_gradcam.png" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org661c45c">
<h3 id="org661c45c">Summary</h3>
<ul>
<li>Computer vision models can learn latent construct like fear <b>(Q1)</b></li>
<li>Explanations for model’s judgments often, but not always, understandable (fear-eliciting stimulus not highlighted) <b>(Q2)</b></li>
<li>Pending tasks:
<ol>
<li>Explore more architectures</li>
<li>Error analysis</li>
<li>Learning curve analysis</li>
<li>Investigate the overlap between the model's and human judgments</li>

</ol></li>

</ul>
</section>
</section>
<section>
<section id="slide-org54d905c">
<h2 id="org54d905c">#3</h2>
<p>
<b>LEARNING OPTIMAL EXPOSURE THERAPY PROTOCOLS WITH REINFORCEMENT LEARNING</b>
</p>


<div id="org8850e4c" class="figure">
<p class="org-p-fig"><img src="./img/p3.png" alt="p3.png" height="400px" />
</p>
</div>

<span style="font-size: 1.33rem">PI: Filip Melinscak</span>
</section>
<section id="slide-org7e53127">
<h3 id="org7e53127">Problem Setting</h3>
<ul>
<li>&#x2026;</li>
<li>&#x2026;</li>
<li>Reinforcement Learning (RL; <a href="#citeproc_bib_item_8">Sutton &#38; Barto, 2018</a>)</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgb3eb66c">
<h2 id="orgb3eb66c">Schedule</h2>

<div id="orgdc94a44" class="figure">
<p class="org-p-fig"><img src="./svg/gantt.svg" alt="gantt.svg" class="org-svg" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-orgce9a7aa">
<h2 id="orgce9a7aa">Summary</h2>
<ul>
<li>&#x2026;</li>
<li>&#x2026;</li>
<li>&#x2026;</li>

</ul>
</section>
</section>
<section>
<section id="slide-bibliography">
<h2 id="bibliography">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Cohen, J. (1988). <i>Statistical Power Analysis for the Behavioral Sciences</i> (2nd Edition). Routledge. <a href="https://doi.org/10.4324/9780203771587">https://doi.org/10.4324/9780203771587</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Deng, J., Dong, W., Socher, R., Li, L.-J., Kai Li, &#38; Li Fei-Fei. (2009). ImageNet: A large-scale hierarchical image database. <i>2009 IEEE Conference on Computer Vision and Pattern Recognition</i>, 248–255. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>He, K., Zhang, X., Ren, S., &#38; Sun, J. (2015). Deep residual learning for image recognition. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 770–778. <a href="https://doi.org/10.48550/arXiv.1512.03385">https://doi.org/10.48550/arXiv.1512.03385</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>LeCun, Y., Bengio, Y., &#38; Hinton, G. (2015). Deep learning. <i>Nature</i>, <i>521</i>(7553, 7553), 436–444. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Picard, R. W. (1997). <i>Affective computing</i>. MIT press. <a href="https://doi.org/10.7551/mitpress/1140.001.0001">https://doi.org/10.7551/mitpress/1140.001.0001</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Sanfeliu, A., &#38; Fu, K.-S. (1983). A distance measure between attributed relational graphs for pattern recognition. <i>Ieee Transactions on Systems, Man, and Cybernetics</i>, <i>SMC-13</i>(3), 353–362. IEEE Transactions on Systems, Man, and Cybernetics. <a href="https://doi.org/10.1109/TSMC.1983.6313167">https://doi.org/10.1109/TSMC.1983.6313167</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a>Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &#38; Batra, D. (2020). Grad-CAM: Visual explanations from deep networks via gradient-based localization. <i>International Journal of Computer Vision</i>, <i>128</i>(2), 336–359. <a href="https://doi.org/10.1007/s11263-019-01228-7">https://doi.org/10.1007/s11263-019-01228-7</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a>Sutton, R. S., &#38; Barto, A. G. (2018). <i>Reinforcement learning: An introduction, 2nd ed.</i> (p. xxii, 526). The MIT Press.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_9"></a>Yosinski, J., Clune, J., Bengio, Y., &#38; Lipson, H. (2014). How transferable are features in deep neural networks? <i>Advances in Neural Information Processing Systems</i>, 3320–3328. <a href="https://doi.org/10.48550/arXiv.1411.1792">https://doi.org/10.48550/arXiv.1411.1792</a></div>
</div>
</section>
</section>
</div>
</div>
<script src="./reveal.js/dist/reveal.js"></script>
<script src="./reveal.js/plugin/markdown/markdown.js"></script>
<script src="./reveal.js/plugin/notes/notes.js"></script>
<script src="./reveal.js/plugin/search/search.js"></script>
<script src="./reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 't',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
width: 1200,
height: 800,
margin: 0.10,

transition: 'slide',
transitionSpeed: 'fast',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
