<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Machine Learning for Human-Centered Solutions</title>
<meta name="author" content="Dominik Pegler"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/white.css" id="theme"/>

<link rel="stylesheet" href="css/custom.css"/>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<div><h1>Machine Learning for Human-Centered Solutions</h1><h3 style="color:#6b6b6b">Interpretability, Emotion Recognition, and Therapeutic Innovation<h3><p style="text-transform:none;color:black;font-weight:normal">Dominik Pegler<vvp></div>
</section>
<section>
<section id="slide-org3b5d5e3">
<h2 id="org3b5d5e3">BACKGROUND</h2>
<div style="display: flex; flex-direction: column">
<div style="display: flex; flex-direction: row;">
<div style="width:60%">

<p>
<b>Rapidly Evolving Landscape of Technology</b>
</p>

<ul>
<li>Reshapes communication, work, and leisure</li>
<li>Human-Computer Interaction (HCI) ensures accessibility and efficiency</li>
<li>Challenges (e.g., Trust and Transparency, Emotional Intelligence)</li>

</ul>

</div>
<div style="width:40%">
<p>
 </p><div class="imgcontainer"><div about="./img/./hci_model.png" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./hci_model.png" alt="Model of HCI" style="max-height:320px" /></p><p property="schema:caption">Model of HCI</p><p style="width:'526'px;max-width:320px">&ldquo;<span property="dcterms:title">Model of HCI</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://commons.wikimedia.org/wiki/File:Basic_model_of_HCI.png#filehistory" property="cc:attributionName">Dr. Greywolf</a> under <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 International</a>; from <a rel="dcterms:source" href="https://commons.wikimedia.org/wiki/File:Basic_model_of_HCI.png">Wikimedia Commons</a></p></div></div><p>
</p>
</div>
</div>
</div>

<p class="fragment (t)">
<b><b>&rarr; Improve HCI with focus on cognitive and affective aspects</b></b>
</p>
</section>
<section id="slide-org4c9811d">
<h3 id="org4c9811d">Cognitive Domain</h3>
<div style="float:left;width:65%;">
<p>
<b>Joint Human-Machine Problem Solving</b>
</p>
<ul>
<li class="fragment"><b>Examples</b>: Cobots (collaborative robots) in engineering and surgery, autopilot systems, self-driving cars</li>
<li class="fragment"><b>Project Focus</b>
<ul>
<li><b>Interaction Setting</b>: Problem-solving game</li>
<li><b>Improvement</b>: Enhance trust by focusing on interpretability</li>

</ul></li>

</ul>
</div>

<div style="float:right;width:35%;">
<p>
 </p><div class="imgcontainer"><div about="./img/./cobot.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./cobot.jpg" alt="Collaborative Robot" style="max-height:40rh" /></p><p property="schema:caption"></p><p style="width:'852'px;max-width:40rh">&ldquo;<span property="dcterms:title">Collaborative Robot</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://commons.wikimedia.org/wiki/File:Db_tuda_jes2899_a.jpg#filehistory" property="cc:attributionName">TSorg</a> under <a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/de">CC BY-SA 3.0 Germany</a>; from <a rel="dcterms:source" href="https://commons.wikimedia.org/wiki/File:Db_tuda_jes2899_a.jpg">Wikimedia Commons</a></p></div></div><p>
</p>
</div>
</section>
<section id="slide-org198e655" data-auto-animate>
<h3 id="org198e655">Affective Domain 1/2</h3>
<div style="float:left;width:75%;">
<p>
For effective and natural human-computer interactions, computers must not only <b><span class="underline">recognize</span></b> but also respond to <b>human emotions</b> (Affective Computing;  <a href="#citeproc_bib_item_6">Picard, 1997</a>)
</p>

<ul>
<li class="fragment"><b>Examples</b>: Interfaces with emotional content, e.g., images on websites, mobile apps</li>
<li class="fragment"><b>Project Focus</b>
<ul>
<li><b>Interaction Setting</b>: Computerized exposure therapy</li>
<li><b>Improvement</b>: Automatic recognition of fear potential in images</li>

</ul></li>

</ul>
</div>

<div style="float:right;width:25%;">
<p>
 </p><div class="imgcontainer"><div about="./img/./affective_computing.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./affective_computing.jpg" alt="Affective Computing." style="max-height:45rh" /></p><p property="schema:caption">Affective Computing (1997) by Rosalind Picard.</p><p style="width:'1956'px;max-width:45rh">&ldquo;<span property="dcterms:title">Affective Computing (1997).</span>&rdquo;  under All rights reserved or Unknown licensing information; from <a rel="dcterms:source" href="https://cdn2.penguin.com.au/covers/original/9780262661157.jpg">https://www.penguin.com.au/</a></p></div></div><p>
</p>
</div>
</section>
<section id="slide-org7932e66" data-auto-animate>
<h3 id="org7932e66">Affective Domain 2/2</h3>
<div style="float:left;width:75%;">
<p>
For effective and natural human-computer interactions, computers must not only recognize but also <b><span class="underline">respond</span></b> to <b>human emotions</b> (Affective Computing;  <a href="#citeproc_bib_item_6">Picard, 1997</a>)
</p>

<ul>
<li class="fragment"><b>Examples</b>: Recommender systems, care robot, counseling chatbot</li>
<li class="fragment"><b>Project Focus</b>
<ul>
<li><b>Interaction Setting</b>: Computerized exposure therapy</li>
<li><b>Improvement</b>: Optimize sequence of stimulus presentation in response to patient's state</li>

</ul></li>

</ul>
</div>

<div style="float:right;width:25%;">
<p>
 </p><div class="imgcontainer"><div about="./img/./affective_computing.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./affective_computing.jpg" alt="Affective Computing." style="max-height:45rh" /></p><p property="schema:caption">Affective Computing (1997) by Rosalind Picard.</p><p style="width:'1956'px;max-width:45rh">&ldquo;<span property="dcterms:title">Affective Computing (1997).</span>&rdquo;  under All rights reserved or Unknown licensing information; from <a rel="dcterms:source" href="https://cdn2.penguin.com.au/covers/original/9780262661157.jpg">https://www.penguin.com.au/</a></p></div></div><p>
</p>
</div>
</section>
<section id="slide-orgb28d393">
<h3 id="orgb28d393">Overview</h3>
<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;"><b>Enhancing Human-Computer Interaction (HCI)</b></div>

<div style="display:flex;flex-direction:column;font-size:2.0rem">

<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:8px;margin-bottom:4px;padding:8px">

<div style="color:#7e7e7e;font-weight:bold;font-size:1.4rem;margin-bottom:4px">Cognitive Domain: Problem Solving</div>

<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

<div style="width:15%;">

<div id="org9babcfd" class="figure">
<p class="org-p-fig"><img src="./img/p1_update.png" alt="p1_update.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">Joint Human-Machine Problem Solving</div>
<div style="font-size:1.6rem;text-align:left">Addressing trust through human interpretability</div>

</div>
</div>

</div>

<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:4px;margin-bottom:0px;padding:8px">

<div style="color:#7e7e7e;font-size:1.4rem;font-weight:bold;margin-bottom:4px">Affective Domain: Phobias</div>

<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

<div style="width:15%;">

<div id="org8c136b8" class="figure">
<p class="org-p-fig"><img src="./img/p2_crop.png" alt="p2_crop.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">Enhancing Interaction through Emotional Awareness</div>


<div style="font-size:1.6rem;text-align:left">Recognizing fear potential in images</div>

</div>
</div>

<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

<div style="width:15%;">

<div id="org1270142" class="figure">
<p class="org-p-fig"><img src="./img/p3_other.png" alt="p3_other.png" height="100px" />
</p>
</div>
</div>

<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

<div style="font-weight:bold;">Optimal Interaction in Therapy</div>

<div style="font-size:1.6rem;text-align:left">Selecting  stimulus images based on patient's state</div>

</div>
</div>


</div>

</div>
</section>
</section>
<section>
<section id="slide-org0be0761">
<h2 id="org0be0761">JOINT HUMAN-MACHINE PROBLEM SOLVING</h2>

<div id="org1b82a57" class="figure">
<p class="org-p-fig"><img src="./img/p1_update.png" alt="p1_update.png" height="300px" />
</p>
</div>
<span style="font-size: 1.2rem">PIs: Frank Scharnowski (<a href="mailto:frank.scharnowski@univie.ac.at">frank.scharnowski@univie.ac.at)</a>, David Steyrl (<a href="mailto:david.steyrl@univie.ac.at">david.steyrl@univie.ac.at)</a> & Filip Melinscak (<a href="mailto:filip.melinscak@univie.ac.at">filip.melinscak@univie.ac.at)</a></span>
</section>
<section id="slide-org79d50a3">
<h3 id="org79d50a3">Problem Setting</h3>
<div style="display: flex; flex-direction: column">
<div style="display: flex; flex-direction: row;">
<div style="width:80%">
<p>
<b>Machine Problem-Solving</b>
</p>
<ul>
<li class="fragment">Increasingly taking over human domains</li>
<li class="fragment">AI getting more complex &rarr; black boxes &rarr; lack of trust</li>
<li class="fragment">Trust issues not new (Classical AI in 1950s)</li>

</ul>
</div>
<div style="width:20%">
<p>
 </p><div class="imgcontainer"><div about="./img/./dantzig_c.jpg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./dantzig_c.jpg" alt="Goerge Dantzig" style="max-height:304px" /></p><p property="schema:caption">George Dantzig &amp; Linear Programming (1947)</p><p style="width:'1881'px;max-width:304px">&ldquo;<span property="dcterms:title">George Dantzig.</span>&rdquo;  under <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a>; from <a rel="dcterms:source" href="https://malevus.com/george-dantzig/">malevus.com</a></p></div></div><p>
</p>
</div>
</div>

<div style="display: flex; flex-direction: row;">
<div style="width:80%">
<p class="fragment (t)">
<b>Evaluating Human Interpretability</b>
</p>
<ul>
<li class="fragment">Human-in-the-loop approach to evaluate interpretability</li>
<li class="fragment">Understanding how a machine makes a decision</li>
<li class="fragment">Critical for trust and collaboration with machines</li>

</ul>
</div>
<div style="width:20%">
<p class="fragment (t)">
 </p><div class="imgcontainer"><div about="./svg/human_loop.svg" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./svg/human_loop.svg" alt="Human-in-the-loop" style="max-height:304px" /></p><p property="schema:caption">Human-in-the-loop.</p><p style="width:'540'px;max-width:304px">&ldquo;<span property="dcterms:title">Human-in-the-loop</span>&rdquo; by <a rel="cc:attributionURL dcterms:creator" href="https://docs.google.com" property="cc:attributionName">Dominik Pegler</a> under <a rel="license" href="https://creativecommons.org/licenses/by-sa/2.5/">CC BY-SA 2.5 Generic</a>; from <a rel="dcterms:source" href="https://docs.google.com">Created using Google Docs</a></p></div></div><p>
</p>
</div>
</div>
</div>
</section>
<section id="slide-orgc270815">
<h3 id="orgc270815">Bin-Packing Problem</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:70%;">
<ul>
<li>Abstract representation of real-world scenarios (e.g., scheduling)</li>
<li>Pack items into boxes</li>
<li>Goal: Fill the boxes as much as possible</li>
<li>Constraint: You cannot overfill the boxes</li>

</ul>
</div>
<div style="display:flex;flex-direction:column;max-width:30%">

<div id="org1da6ffa" class="figure">
<p class="org-p-fig"><img src="./img/binpacking.gif" alt="binpacking.gif" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 1: </span>A human performing the bin packing task.</p>
</div>
</div>
</div>
</section>
<section id="slide-org6da08ab">
<h3 id="org6da08ab">Optimal Solutions</h3>

<div id="orgdbf604b" class="figure">
<p class="org-p-fig"><img src="./img/optimalsolutions.gif" alt="optimalsolutions.gif" height="600px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 2: </span>The machine (<a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>) providing possible optimal solutions.</p>
</div>
</section>
<section id="slide-org9e405f1">
<h3 id="org9e405f1">Question</h3>
<p>
<b>"What makes a solution interpretable?"</b>
</p>
</section>
<section id="slide-org61c82ca">
<h3 id="org61c82ca">H1: Heuristic</h3>
<ul>
<li>Humans use (greedy) heuristics &rarr; greedy solution</li>
<li>Similarity to greedy solution is measured by graph edit distance(<a href="#citeproc_bib_item_8">Sanfeliu &#38; Fu, 1983</a>)</li>

</ul>


<div id="orgb3167f2" class="figure">
<p class="org-p-fig"><img src="./svg/heuristic.svg" alt="heuristic.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if similar to greedy solution</b>
</p>
</section>
<section id="slide-org080b29b">
<h3 id="org080b29b">H2: Simplicity</h3>
<ul>
<li>Bins can look more or less simple/complex</li>
<li>Formalized as log-probability that a mixture model (2 dirichlet, 1 geometric distribution) returns for each bin composition</li>

</ul>


<div id="orgc1aeba9" class="figure">
<p class="org-p-fig"><img src="./svg/composition.svg" alt="composition.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if simple</b>
</p>
</section>
<section id="slide-org5958fb7">
<h3 id="org5958fb7">H3: Representation</h3>
<ul>
<li>Items and boxes can be sorted by size or at random</li>
<li>Formalized as rank correlation between the actual order and the sorted order</li>

</ul>


<div id="org0e203d4" class="figure">
<p class="org-p-fig"><img src="./svg/order.svg" alt="order.svg" class="org-svg" height="480px" margin-top="0px" margin-bottom="0px" />
</p>
</div>

<p class="fragment (t)">
<b>&rarr; Solutions more interpretable if sorted</b>
</p>
</section>
<section id="slide-orgbfc9408" data-auto-animate>
<h3 id="orgbfc9408">Online-Experiment</h3>
<img src="svg/experiment_1.svg" alt="experiment overview" style="max-height:666px"/>
<p>
<i>N</i> = 73 participants (pilot)
</p>
</section>
<section id="slide-org0f299d2" data-auto-animate>
<h3 id="org0f299d2">Online-Experiment</h3>
<img src="svg/experiment_2.svg" alt="experiment detail" style="max-height:666px"/>
<p>
<i>N</i> = 73 participants (pilot)
</p>
</section>
<section id="slide-org0e98261">
<h3 id="org0e98261">Pilot Results</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:60%;">


<div id="orgc9d9233" class="figure">
<p class="org-p-fig"><img src="./img/results_choice_fixed_effects.png" alt="results_choice_fixed_effects.png" height="100%" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 3: </span>Fixed Effects Estimates of Predictor Variables (x-axis) on Choice (y-axis) in Multilevel Analysis. The plot displays the estimated fixed effects (with 95% confidence intervals) for the three predictors. The effects are adjusted for random effects at the group level.</p>
</div>
</div>

<div style="display:flex; flex-direction:column;justify-content:center;width:40%;">
<ul>
<li><b>All three</b> predictors relevant for people's choices</li>
<li><b>Order</b> and <b>Heuristic</b> most influental</li>
<li><b>Explained variance</b>: <i>R²</i>=0.17</li>

</ul>
<div style="font-size:1.5rem;">

</div>
</div>
</div>
</section>
<section id="slide-org93162ab">
<h3 id="org93162ab">Limitations</h3>
<ol>
<li>Did we measure <b>interpretability</b>?</li>
<li>Focus on <b>small problems</b> (diversity of solutions limited)</li>
<li>Only tested for optimal solutions, <b>no suboptimal solutions</b></li>

</ol>

<p class="fragment (t)">
&rarr; Follow-up publication, e.g., with collaboration task to address limitation 1
</p>
</section>
<section id="slide-orgd1d166e">
<h3 id="orgd1d166e">Current Status</h3>
<ul>
<li><b>Completed</b>: Experimental design, analysis pipeline and pilot data collection</li>

<li><b>Pending</b>:

<ol>
<li>Preregistration</li>
<li>Confirmatory data collection &amp; analysis</li>
<li>Write draft</li>

</ol></li>

<li><b>Publication</b>: Early 2025</li>

<li><b>Target journals</b>: <i>International Journal of Human-Computer Studies, Computers in Human Behavior, Behaviour and Information Technology</i></li>

</ul>
</section>
<section id="slide-orgcfff556">
<h3 id="orgcfff556">Takeaways</h3>
<ul>
<li>Humans seem to use <b>solving heuristics during evaluation</b>​</li>
<li>Adequate <b>visual representation</b> is requirement​</li>
<li>All factors may play a bigger role in <b>larger problems</b>​</li>
<li><b>Validation</b> required</li>

</ul>


<div>
<div style="font-size:0.8rem;margin-top:2rem;margin-bottom:1rem;">This project is funded by <a href="https://www.ffg.at">www.ffg.at</a></div>

<div id="org1afd6d3" class="figure">
<p class="org-p-fig"><img src="./svg/ffg_logo_en.svg" alt="ffg_logo_en.svg" class="org-svg" height="40px" />
</p>
</div>
</div>

<div>
<div style="font-size:0.8rem;margin-top:2rem;margin-bottom:1rem">and carried out in association with</div>
<div style="display:flex;justify-content:center;align-items:center;">


<div id="org2dc9271" class="figure">
<p class="org-p-fig"><img src="./svg/UniWien_CMYK_A4.svg" alt="UniWien_CMYK_A4.svg" class="org-svg" width="130px" />
</p>
</div>


<div id="org3b26ffa" class="figure">
<p class="org-p-fig"><img src="./svg/tu_logo.svg" alt="tu_logo.svg" class="org-svg" height="50vpx" />
</p>
</div>


<div id="orgb799f7d" class="figure">
<p class="org-p-fig"><img src="./img/tttech_logo.png" alt="tttech_logo.png" width="120px" />
</p>
</div>

</div>
</div>
</section>
</section>
<section>
<section id="slide-orgaac981d">
<h2 id="orgaac981d">INTERACTION WITH EMOTIONAL AWARENESS</h2>

<div id="org367970e" class="figure">
<p class="org-p-fig"><img src="./img/p2_crop.png" alt="p2_crop.png" height="300px" />
</p>
</div>
</section>
<section id="slide-orgd391ecd">
<h3 id="orgd391ecd">Problem Setting</h3>
<div style="display:flex;flex-direction:column;height: 100%">
<div style="display:flex; flex-direction:row;justify-content:space-evenly;height:40%;">


<div id="orgcd2009f" class="figure">
<p class="org-p-fig"><img src="./img/example_stimuli_large.png" alt="example_stimuli_large.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 4: </span>The stimulus set.</p>
</div>


<div id="org4006b1e" class="figure">
<p class="org-p-fig"><img src="./img/fear_ratings.png" alt="fear_ratings.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 5: </span>Rating the fear level of each image.</p>
</div>


<div id="org23f8181" class="figure">
<p class="org-p-fig"><img src="./img/example_stimuli_rated.png" alt="example_stimuli_rated.png" height="250px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 6: </span>Example rated images.</p>
</div>

</div>
<div style="display:flex;flex-direction:column;max-height:60%">

<ul>
<li class="fragment"><b>Interaction Setting</b>: Computerized Exposure Therapy</li>
<li class="fragment"><b>Images with emotional content</b> (e.g., spiders)
<ul>
<li>Usage requires information, e.g., how much fear they provoke</li>
<li>Fear ratings for 313 spider images (<a href="#citeproc_bib_item_3">Karner et al., 2024</a>)</li>

</ul></li>

</ul>
<ul>
<li class="fragment"><b>Improvement</b>: Automatic evaluation of fear potential in new images</li>

</ul>
</section>
<section id="slide-org1809d50">
<h3 id="org1809d50">Research questions</h3>
<div style="border-radius:12px;background-color:lightblue;">
<p class="fragment (t)">
Q1: Can a machine learning model built for object recognition learn a <b>latent construct</b> such as fear?
</p>
<div style="font-size:1.9rem;">
<p class="fragment (t)">
Q1-1: How much <b>data</b> do we need?
</p>
<p class="fragment (t)">
Q1-2: What <b>erros</b> will it make?
</p>
</div>
</div>

<div style="border-radius:12px;background-color:lightblue;margin-top:3rem">
<p class="fragment (t)">
Q2: <b>How</b> does the model arrive at its judgments and how do they differ from <b>human judgments</b>?
</p>
</div>
</section>
<section id="slide-orge4b2320" data-auto-animate>
<h3 id="orge4b2320">Deep Neural Networks</h3>
<ul>
<li>Use deep neural networks to rate new images (<a href="#citeproc_bib_item_4">LeCun et al., 2015</a>)</li>
<li>Pre-trained on large datasets (ImageNet; <a href="#citeproc_bib_item_1">Deng et al., 2009</a>)</li>

</ul>
<p>
 </p><div class="imgcontainer"><div about="./img/./cnn_architecture.png" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./cnn_architecture.png" alt="CNN" style="max-height:320px" /></p><p property="schema:caption">Architecture of a convolutional neural network (CNN)</p><p style="width:'530'px;max-width:320px">&ldquo;<span property="dcterms:title">CNN architecture.</span>&rdquo;  under <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a>; from <a rel="dcterms:source" href="https://developersbreach.com/convolution-neural-network-deep-learning">developersbreach.com</a></p></div></div><p>
</p>
</section>
<section id="slide-org7335ca6" data-auto-animate>
<h3 id="org7335ca6">Deep Neural Networks</h3>
<ul>
<li>Use deep neural networks to rate new images (<a href="#citeproc_bib_item_4">LeCun et al., 2015</a>)</li>
<li>Pre-trained on large datasets (ImageNet; <a href="#citeproc_bib_item_1">Deng et al., 2009</a>)</li>

</ul>
<p>
 </p><div class="imgcontainer"><div about="./img/./cnn_architecture_mod.png" typeof="schema:ImageObject schema:LearningResource dcmitype:StillImage" class="figure"><p class="p-figure"><img data-src="./img/./cnn_architecture_mod.png" alt="CNN" style="max-height:320px" /></p><p property="schema:caption">Architecture of a convolutional neural network (CNN)</p><p style="width:'530'px;max-width:320px">&ldquo;<span property="dcterms:title">CNN architecture.</span>&rdquo;  under <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a>; from <a rel="dcterms:source" href="https://developersbreach.com/convolution-neural-network-deep-learning">developersbreach.com</a></p></div></div><p>
</p>
<ul>
<li>Transfer Learning (<a href="#citeproc_bib_item_11">Yosinski et al., 2014</a>): Adapt &amp; fine-tune on own data (313 images with fear ratings) &rarr; <b>"Spider-Fear-Network"</b></li>

</ul>
</section>
<section id="slide-orgcc20ad5">
<h3 id="orgcc20ad5">First Results</h3>
</section>
<section id="slide-org74113b3">
<h3 id="org74113b3">Predictions</h3>
<div style="display:flex;flex-direction:row;width: 100%">
<div style="display:flex; flex-direction:column;justify-content:center;width:60%;">

<div id="org648637f" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_results.svg" alt="cnn_results.svg" class="org-svg" height="600px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 7: </span>Predictive performance of the CNN model for each image.</p>
</div>
</div>

<div style="display:flex; flex-direction:column;justify-content:flex-start;width:40%;">
<ul>
<li>Promising predictive accuracy</li>
<li>Model: ResNet50 (<a href="#citeproc_bib_item_2">He et al., 2015</a>)</li>
<li>Training is possible on standard PC hardware (hours–days)</li>

</ul>
</div>
</div>
</section>
<section id="slide-org6014f2a">
<h3 id="org6014f2a">Attributions</h3>

</section>
<section id="slide-orgba4765c" data-auto-animate>
<h3 id="orgba4765c">Attributions</h3>
<p>
Gradient-weighted Class Activation Mapping (Grad-CAM; <a href="#citeproc_bib_item_9">Selvaraju et al., 2020</a>)
</p>


<div id="org43675c9" class="figure">
<p class="org-p-fig"><img src="./img/Sp_283_gradcam.png" alt="Sp_283_gradcam.png" />
</p>
</div>
</section>
<section id="slide-org0e910ce" data-auto-animate>
<h3 id="org0e910ce">Attributions</h3>
<p>
Gradient-weighted Class Activation Mapping (Grad-CAM; <a href="#citeproc_bib_item_9">Selvaraju et al., 2020</a>)
</p>


<div id="org2f25b7b" class="figure">
<p class="org-p-fig"><img src="./img/Sp_111_gradcam.png" alt="Sp_111_gradcam.png" />
</p>
</div>
</section>
<section id="slide-org0a79555" data-auto-animate>
<h3 id="org0a79555">Attributions</h3>
<p>
Gradient-weighted Class Activation Mapping (Grad-CAM; <a href="#citeproc_bib_item_9">Selvaraju et al., 2020</a>)
</p>


<div id="org4868408" class="figure">
<p class="org-p-fig"><img src="./img/Sp_285_gradcam.png" alt="Sp_285_gradcam.png" />
</p>
</div>
</section>
<section id="slide-orgb5f07da" data-auto-animate>
<h3 id="orgb5f07da">Attributions</h3>
<p>
Gradient-weighted Class Activation Mapping (Grad-CAM; <a href="#citeproc_bib_item_9">Selvaraju et al., 2020</a>)
</p>


<div id="org361ced0" class="figure">
<p class="org-p-fig"><img src="./img/Sp_073_gradcam.png" alt="Sp_073_gradcam.png" />
</p>
</div>
</section>
<section id="slide-org5d36647">
<h3 id="org5d36647">Feature Visualization</h3>
</section>
<section id="slide-org7478dd0" data-auto-animate>
<h3 id="org7478dd0">Feature Visualization</h3>
<p>
What does a neuron in each layer look for?
</p>


<div id="orgbd3994d" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_layers_early.svg" alt="cnn_layers_early.svg" class="org-svg" height="70px" />
</p>
</div>

<p>
<b>Early Layers</b>
</p>

<div style="display:flex;flex-direction:row">

<div id="org04a142d" class="figure">
<p class="org-p-fig"><img src="./img/features_early.png" alt="features_early.png" width="100%" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 8: </span>Activation patterns in 9 example neurons for layer 1 (left), layer 2.1 (middle) and layer 2.2 (right).</p>
</div>
</section>
<section id="slide-org538230a" data-auto-animate>
<h3 id="org538230a">Feature Visualization</h3>
<p>
What does a neuron in each layer look for?
</p>


<div id="org5f70dec" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_layers_intermediate.svg" alt="cnn_layers_intermediate.svg" class="org-svg" height="70px" />
</p>
</div>

<p>
<b>Intermediate Layers</b>
</p>

<div style="display:flex;flex-direction:row">

<div id="orgc4f83a5" class="figure">
<p class="org-p-fig"><img src="./img/features_intermediate.png" alt="features_intermediate.png" width="100%" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 9: </span>Activation patterns in example neurons for layer 3.1 (left), layer 3.2 (middle) and layer 3.3 (right).</p>
</div>
</div>
</section>
<section id="slide-orgdc8636b" data-auto-animate>
<h3 id="orgdc8636b">Feature Visualization</h3>
<p>
What does a neuron in each layer look for?
</p>


<div id="orgadb605b" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_layers_last.svg" alt="cnn_layers_last.svg" class="org-svg" height="70px" />
</p>
</div>

<p>
<b>Last Layers</b>
</p>

<div style="display:flex;flex-direction:row">

<div id="org321b650" class="figure">
<p class="org-p-fig"><img src="./img/features_last.png" alt="features_last.png" width="100%" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 10: </span>Activation patterns in example neurons for layer 4.1 (left), layer 4.2 (middle) and layer 4.3 (right).</p>
</div>
</div>
</section>
<section id="slide-org74466c0" data-auto-animate>
<h3 id="org74466c0">Feature Visualization</h3>
<p>
What does a neuron in each layer look for?
</p>


<div id="org7560e53" class="figure">
<p class="org-p-fig"><img src="./svg/cnn_layers_final.svg" alt="cnn_layers_final.svg" class="org-svg" height="70px" />
</p>
</div>

<p>
<b>Final Output Node</b>
</p>


<div id="org2fc6f04" class="figure">
<p class="org-p-fig"><img src="./img/features_final.png" alt="features_final.png" width="100%" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 11: </span>Three example images that maximize the final output "Fear".</p>
</div>
</section>
<section id="slide-orgc2b2531">
<h3 id="orgc2b2531">Current Status</h3>
<ul>
<li><b>Completed</b>: Concept, Analysis Pipeline</li>
<li><b>Pending</b>:
<ul>
<li>Explore more architectures</li>
<li>Error &amp; learning curve analysis</li>
<li>Investigate overlap with human judgments</li>
<li>Write draft</li>

</ul></li>
<li><b>Publication</b>: 2025</li>
<li><b>Target journals</b>: <i>International Journal of Human-Computer Studies, IEEE Transactions on Affective Computing, Computers in Human Behavior, Behaviour and Information Technology</i></li>

</ul>
</section>
<section id="slide-orgc98937e">
<h3 id="orgc98937e">Takeaways</h3>
<ul>
<li>Computer vision models can learn latent construct like fear <b>(Q1)</b></li>
<li>Model’s judgments often, but not always, understandable (fear-eliciting stimulus not highlighted) <b>(Q2)</b></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgfd9149e">
<h2 id="orgfd9149e">OPTIMAL INTERACTION IN EXPOSURE THERAPY</h2>

<div id="org9823090" class="figure">
<p class="org-p-fig"><img src="./img/p3_other.png" alt="p3_other.png" height="300px" />
</p>
</div>
<span style="font-size: 1.2rem">PI: Filip Melinscak (<a href="mailto:filip.melinscak@univie.ac.at">filip.melinscak@univie.ac.at)</a></span>
</section>
<section id="slide-org0d12381">
<h3 id="org0d12381">Problem Setting</h3>
<div style="float:right;width:75%;">
<ul>
<li class="fragment"><b>Interaction Setting</b>: Computerized Exposure Therapy</li>
<li class="fragment"><b>Improvement</b>: Find optimal stimulus sequence based on patient's state</li>
<li class="fragment"><b>Challenges</b>:
<ul>
<li>Highly complex and individualized process</li>
<li>Inconsistent and subjective protocol tailoring</li>
<li>High-dimensional variable space</li>

</ul></li>

</ul>
</div>

<div style="float:right;width:25%;">

<div id="org0223175" class="figure">
<p class="org-p-fig"><img src="./img/p3_sequence.png" alt="p3_sequence.png" height="300px" margin-top="0px" margin-bottom="0px" />
</p>
</div>
</div>
</section>
<section id="slide-orga24a889">
<h3 id="orga24a889">Reinforcement Learning</h3>

<div id="org594d202" class="figure">
<p class="org-p-fig"><img src="./img/aether_illustration.png" alt="aether_illustration.png" height="300px" margin-top="0px" margin-bottom="0px" />
</p>
<p class="org-p-figcaption"><span class="figure-number">Figure 12: </span>Exposure therapy as a reinforcement learning setting.</p>
</div>

<ul>
<li class="fragment"><b>Reinforcement Learning</b> (RL; <a href="#citeproc_bib_item_10">Sutton &#38; Barto, 2018</a>) offers a data-driven approach</li>
<li class="fragment">Agent learns optimal actions through <b>trial and error</b></li>
<li class="fragment"><b>Therapist</b>: Deep RL Algorithms like Deep Q Networks (DQN; <a href="#citeproc_bib_item_5">Mnih et al., 2013</a>)</li>
<li class="fragment"><b>Simulated Patient</b>: e.g., Rescorla &#38; Wagner (<a href="#citeproc_bib_item_7">1972</a>) \(F_{\text{expected}} \leftarrow F_{\text{expected}} + \alpha (F_{\text{actual}} - F_{\text{expected}})\)</li>

</ul>
</section>
<section id="slide-org0aa5507" data-auto-animate>
<h3 id="org0aa5507">Research Questions</h3>
<div style="border-radius:12px;background-color:lightblue;">
<p class="fragment (t)">
Q1: How can RL <b>model fear extinction</b> and optimize therapy protocols?
</p>
</div>

<div style="border-radius:12px;background-color:lightblue;margin-top:3rem">
<p class="fragment (t)">
Q2: Can RL improve <b>consistency and objectivity</b> in exposure therapy?
</p>
</div>
</section>
<section id="slide-org53c35f8" data-auto-animate>
<h3 id="org53c35f8">Planned Methodology</h3>
<div style="font-size:2.6rem;border-radius:12px;background-color:lightblue;">
<p>
Q1: How can RL <b>model fear extinction</b> and optimize therapy protocols?
</p>
</div>

<div style="font-size:2.4rem;">
<ol>
<li class="fragment">Selecting <b>RL algorithms</b> based on theoretical and practical applicability</li>
<li class="fragment">Conducting preliminary <b>simulations</b> using models such as Rescorla-Wagner</li>
<li class="fragment">Defining and testing <b>reward functions</b> to guide learning</li>
<li class="fragment">Running iterative computational <b>experiments</b> to refine algorithms</li>

</ol>
</div>

<div style="font-size:2.6rem;border-radius:12px;background-color:lightblue;">
<p>
Q2: Can RL improve <b>consistency and objectivity</b> in exposure therapy?
</p>
</div>

<div style="font-size:2.4rem;">
<p class="fragment (t)">
Empirical validation with real subjects
</p>
</div>
</section>
<section id="slide-org16e3d1d">
<h3 id="org16e3d1d">Current Status</h3>
<ul>
<li><b>Completed</b>: Concept (partly)</li>

<li><b>Pending</b>:

<ol>
<li>Literature Review</li>
<li>Select Algorithms</li>
<li>Run Simulations</li>
<li>Design Experiment</li>

</ol></li>

<li><b>Publication</b>: 2026</li>

<li><b>Target journals</b>: <i>International Journal of Human-Computer Studies, IEEE Transactions on Affective Computing, Computers in Human Behavior, Behaviour and Information Technology</i></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgae90c2e">
<h2 id="orgae90c2e">Open Science</h2>

<div id="orge21436b" class="figure">
<p class="org-p-fig"><img src="./img/open_science.jpg" alt="open_science.jpg" height="300px" />
</p>
</div>

<p>
All data, code, material, preregistrations will be made openly available on <a href="https://osf.io/">osf.io</a>
</p>
</section>
</section>
<section>
<section id="slide-orge891eca">
<h2 id="orge891eca">Schedule</h2>
<div style="float:left;width:7%;display:flex;flex-direction:column;justify-content:space-evenly;height:600px">

<div id="org22abf9c" class="figure">
<p class="org-p-fig"><img src="./img/p1_update.png" alt="p1_update.png" max-width="100px" />
</p>
</div>

<div id="org09d0573" class="figure">
<p class="org-p-fig"><img src="./img/p2_crop.png" alt="p2_crop.png" max-width="100px" />
</p>
</div>

<div id="orge876eba" class="figure">
<p class="org-p-fig"><img src="./img/p3_other.png" alt="p3_other.png" max-width="100px" />
</p>
</div>
</div>

<div style="float:right;width:93%;justify-items:flex-start">

<div id="orge6db8cf" class="figure">
<p class="org-p-fig"><img src="./svg/gantt.svg" alt="gantt.svg" class="org-svg" width="1350px" margin-top="0px" />
</p>
</div>
</div>
</section>
</section>
<section>
<section id="slide-org92968ac">
<h2 id="org92968ac">Summary</h2>
<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;"><b>Enhancing Human-Computer Interaction (HCI)</b></div>
<ol>
<li class="fragment">First results show us how we can enhance <b>trust</b> by finding factors that matter for interpretability</li>
<li class="fragment">AI models can learn <b>emotional potential in images</b>, but the "how" remains open</li>
<li class="fragment">Reinforcement learning is a promising approach for finding good <b>interaction protocols</b> in computerized exposure therapy</li>

</ol>
</section>
</section>
<section>
<section id="slide-bibliography">
<h2 id="bibliography">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Deng, J., Dong, W., Socher, R., Li, L.-J., Kai Li, &#38; Li Fei-Fei. (2009). ImageNet: A large-scale hierarchical image database. <i>2009 IEEE Conference on Computer Vision and Pattern Recognition</i>, 248–255. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>He, K., Zhang, X., Ren, S., &#38; Sun, J. (2015). Deep residual learning for image recognition. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 770–778. <a href="https://doi.org/10.48550/arXiv.1512.03385">https://doi.org/10.48550/arXiv.1512.03385</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Karner, A., Zhang, M., Lor, C. S., Steyrl, D., Götzendorfer, S. J., Weidt, S., Melinscak, F., &#38; Scharnowski, F. (2024). The “SpiDa” dataset: Self-report questionnaires and ratings of spider images from spider-fearful individuals. <i>Frontiers in Psychology</i>, <i>15</i>. <a href="https://doi.org/10.3389/fpsyg.2024.1327367">https://doi.org/10.3389/fpsyg.2024.1327367</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>LeCun, Y., Bengio, Y., &#38; Hinton, G. (2015). Deep learning. <i>Nature</i>, <i>521</i>(7553, 7553), 436–444. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &#38; Riedmiller, M. (2013, December 19). <i>Playing atari with deep reinforcement learning</i>. <a href="https://doi.org/10.48550/arXiv.1312.5602">https://doi.org/10.48550/arXiv.1312.5602</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Picard, R. W. (1997). <i>Affective computing</i>. MIT press. <a href="https://doi.org/10.7551/mitpress/1140.001.0001">https://doi.org/10.7551/mitpress/1140.001.0001</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a>Rescorla, R. A., &#38; Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In <i>Classical conditioning II: Current research and theory</i> (pp. 64–99). Appleton-Century-Crofts.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a>Sanfeliu, A., &#38; Fu, K.-S. (1983). A distance measure between attributed relational graphs for pattern recognition. <i>Ieee Transactions on Systems, Man, and Cybernetics</i>, <i>SMC-13</i>(3), 353–362. IEEE Transactions on Systems, Man, and Cybernetics. <a href="https://doi.org/10.1109/TSMC.1983.6313167">https://doi.org/10.1109/TSMC.1983.6313167</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_9"></a>Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &#38; Batra, D. (2020). Grad-CAM: Visual explanations from deep networks via gradient-based localization. <i>International Journal of Computer Vision</i>, <i>128</i>(2), 336–359. <a href="https://doi.org/10.1007/s11263-019-01228-7">https://doi.org/10.1007/s11263-019-01228-7</a></div>
  <div class="csl-entry"><a id="citeproc_bib_item_10"></a>Sutton, R. S., &#38; Barto, A. G. (2018). <i>Reinforcement learning: An introduction, 2nd ed.</i> (p. xxii, 526). The MIT Press.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_11"></a>Yosinski, J., Clune, J., Bengio, Y., &#38; Lipson, H. (2014). How transferable are features in deep neural networks? <i>Advances in Neural Information Processing Systems</i>, 3320–3328. <a href="https://doi.org/10.48550/arXiv.1411.1792">https://doi.org/10.48550/arXiv.1411.1792</a></div>
</div>
</section>
</section>
</div>
</div>
<script src="./reveal.js/dist/reveal.js"></script>
<script src="./reveal.js/plugin/markdown/markdown.js"></script>
<script src="./reveal.js/plugin/notes/notes.js"></script>
<script src="./reveal.js/plugin/search/search.js"></script>
<script src="./reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 't',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
width: 1422,
height: 800,
margin: 0.10,

transition: 'slide',
transitionSpeed: 'fast',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
