:PROPERTIES:
:ID:       dae7ee8b-4424-404a-be4c-df415e5abab7
:END:
#+title: Machine Learning for Human-Centered Solutions
#+subtitle: Interpretability, Emotion Recognition, and Therapeutic Innovation
#+project: Faculty Open Presentation 2024
#+created: [2024-09-26 Thu]
#+last_modified: [2024-09-26 Thu 21:21]
#+author: Dominik Pegler
#+email: dominik.pegler@univie.ac.at
#+date: 2024-09-16
#+REVEAL_THEME: white
#+REVEAL_MARGIN: 0.1
#+REVEAL_TRANS: slide
#+REVEAL_SPEED: fast
# +reveal_slide_footer: <div>Footer</div>
#+reveal_single_file: t
#+OPTIONS: num:nil toc:nil reveal_progress:t reveal_control:t reveal_slide_number:t 
#+OPTIONS: reveal_width:1422 reveal_height:800 reveal_center:t reveal_keyboard:t reveal_overview:t
#+BIBLIOGRAPHY: /home/user/Dropbox/org/ref/ref.bib
#+cite_export: csl apa.csl
#+REVEAL_EXTRA_CSS: css/custom.css
#+REVEAL_TITLE_SLIDE:<div><h1>%t</h1><h3 style="color:#6b6b6b">%s<h3><p style="text-transform:none;color:black;font-weight:normal">%a<vvp></div>
#+MACRO: revealimg (eval (oer-reveal-export-attribution $1 $2 $3 $4 $5 $6))
#+MACRO: reveallicense (eval (oer-reveal-export-attribution $1 nil $2 $3 $4 $5 $6))

* reveal.js infos :noexport:

- https://earvingad.github.io/posts/img/orgreveal/orgreveal.html
- https://github.com/emacsmirror/org-re-reveal
- https://revealjs.com/config/
- https://ertwro.github.io/githubppt/Readmeofficial.html

on how to create reusable css classes
- https://www.gibiris.org/eo-blog/posts/2022/09/28_org-reveal-and-gridded-layouts.html

- TODO: check how to insert image licenses using templates https://oer.gitlab.io/emacs-reveal-howto/howto.html#/slide-figure-with-meta-data
- TODO: how to create simple diagrams with diagram+d3js plugins
- TODO: split bibliography if it gets too long
- TODO: find simpler way to create (css classes?) to particular slide layouts

* BACKGROUND

#+REVEAL_HTML: <div style="display: flex; flex-direction: column">
#+REVEAL_HTML: <div style="display: flex; flex-direction: row;">
#+REVEAL_HTML: <div style="width:60%">

*Rapidly Evolving Landscape of Technology*

- Reshapes communication, work, and leisure
- Human-Computer Interaction (HCI) ensures accessibility and efficiency
- Challenges (e.g., Trust and Transparency, Emotional Intelligence)

# Building reliable and understandable systems.

# Creating systems that recognize and respond to emotions.

#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="width:40%">
{{{revealimg("./img/hci_model.png.meta", "Model of HCI", "40rh")}}}
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+ATTR_REVEAL: :frag (t)
**\to Improve HCI with focus on cognitive and affective aspects**

** Cognitive Domain
#+reveal_html: <div style="float:left;width:65%;">
*Joint Human-Machine Problem Solving*
#+ATTR_REVEAL: :frag (t)
- *Examples*: Cobots (collaborative robots) in engineering and surgery, autopilot systems, self-driving cars
- *Project Focus*
  - *Interaction Setting*: Problem-solving game
  - *Improvement*: Enhance trust by focusing on interpretability
#+reveal_html: </div>

#+reveal_html: <div style="float:right;width:35%;">
{{{revealimg("./img/cobot.jpg.meta", "", 40rh")}}}
#+reveal_html: </div>
    
** Affective Domain 1/2
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
#+reveal_html: <div style="float:left;width:75%;">
For effective and natural human-computer interactions, computers must not only *_recognize_* but also respond to *human emotions* (Affective Computing;  [cite//bare:@picardAffectiveComputing1997])

#+ATTR_REVEAL: :frag (t)
- *Examples*: Interfaces with emotional content, e.g., images on websites, mobile apps
- *Project Focus*
  - *Interaction Setting*: Computerized exposure therapy
  - *Improvement*: Automatic recognition of fear potential in images
#+reveal_html: </div>

#+reveal_html: <div style="float:right;width:25%;">
{{{revealimg("./img/affective_computing.jpg.meta", "Affective Computing (1997) by Rosalind Picard.", 45rh")}}}
#+reveal_html: </div>
** Affective Domain 2/2
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
#+reveal_html: <div style="float:left;width:75%;">
For effective and natural human-computer interactions, computers must not only recognize but also *_respond_* to *human emotions* (Affective Computing;  [cite//bare:@picardAffectiveComputing1997])

#+ATTR_REVEAL: :frag (t)
- *Examples*: Recommender systems, care robot, counseling chatbot
- *Project Focus*
  - *Interaction Setting*: Computerized exposure therapy
  - *Improvement*: Optimize sequence of stimulus presentation in response to patient's state
#+reveal_html: </div>

#+reveal_html: <div style="float:right;width:25%;">
{{{revealimg("./img/affective_computing.jpg.meta", "Affective Computing (1997) by Rosalind Picard.", 45rh")}}}
#+reveal_html: </div>
** Overview

# blue box

#+reveal_html:<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;"><b>Enhancing Human-Computer Interaction (HCI)</b></div>

# outer div

#+reveal_html:<div style="display:flex;flex-direction:column;font-size:2.0rem">

# grey box 1

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:8px;margin-bottom:4px;padding:8px">

# header 1

#+reveal_html:<div style="color:#7e7e7e;font-weight:bold;font-size:1.4rem;margin-bottom:4px">Cognitive Domain: Problem Solving</div>

# item 1-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p1_update.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Joint Human-Machine Problem Solving</div>
#+reveal_html:<div style="font-size:1.6rem;text-align:left">Addressing trust through human interpretability</div>

# end of item 1-1
#+reveal_html:</div>
#+reveal_html:</div>

# end of grey box 1
#+reveal_html:</div>

# grey box 2

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:4px;margin-bottom:0px;padding:8px">

# header 2

#+reveal_html:<div style="color:#7e7e7e;font-size:1.4rem;font-weight:bold;margin-bottom:4px">Affective Domain: Phobias</div>

# item 2-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p2_crop.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Enhancing Interaction through Emotional Awareness</div>


#+reveal_html:<div style="font-size:1.6rem;text-align:left">Recognizing fear potential in images</div>

# end of item 2-1
#+reveal_html:</div>
#+reveal_html:</div>

# item 2-2

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p3_other.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Optimal Interaction in Therapy</div>

#+reveal_html:<div style="font-size:1.6rem;text-align:left">Selecting  stimulus images based on patient's state</div>

# end of item 2-2
#+reveal_html:</div>
#+reveal_html:</div>


# end of grey box 2
#+reveal_html:</div>

# end of outer div
#+reveal_html:</div>

** Overview :noexport:
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

# blue box

#+reveal_html:<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;"><b>Enhancing Human-Computer Interaction (HCI)</b></div>

# outer div

#+reveal_html:<div style="display:flex;flex-direction:column;font-size:2.0rem">

# grey box 1

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:8px;margin-bottom:4px;padding:8px">

# header 1

#+reveal_html:<div style="color:#7e7e7e;font-weight:bold;font-size:1.4rem;margin-bottom:4px">Cognitive Domain: Problem Solving</div>

# item 1-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p1_update.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Joint Human-Machine Problem Solving</div>
#+reveal_html:<div style="font-size:1.6rem;text-align:left">Addressing trust through human interpretability</div>

# end of item 1-1
#+reveal_html:</div>
#+reveal_html:</div>

# end of grey box 1
#+reveal_html:</div>

# grey box 2

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:4px;margin-bottom:0px;padding:8px">

# header 2

#+reveal_html:<div style="color:#7e7e7e;font-size:1.4rem;font-weight:bold;margin-bottom:4px">Affective Domain: Phobias</div>

# item 2-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p2_crop.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Enhancing Interaction through Emotional Awareness</div>


#+reveal_html:<div style="font-size:1.6rem;text-align:left">Awareness of fear potential in images using artificial neural networks</div>

# end of item 2-1
#+reveal_html:</div>
#+reveal_html:</div>

# item 2-2

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem; width:100%">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p3_other.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">Optimal Interaction in Therapy</div>

#+reveal_html:<div style="font-size:1.6rem;text-align:left">Selecting  stimulus images based on patient's state</div>

# end of item 2-2
#+reveal_html:</div>
#+reveal_html:</div>


# end of grey box 2
#+reveal_html:</div>

# end of outer div
#+reveal_html:</div>

* JOINT HUMAN-MACHINE PROBLEM SOLVING

#+ATTR_HTML: :height 300px
file:./img/p1_update.png
#+REVEAL_HTML: <span style="font-size: 1.2rem">PIs: Frank Scharnowski (<a href="mailto:frank.scharnowski@univie.ac.at">frank.scharnowski@univie.ac.at)</a>, David Steyrl (<a href="mailto:david.steyrl@univie.ac.at">david.steyrl@univie.ac.at)</a> & Filip Melinscak (<a href="mailto:filip.melinscak@univie.ac.at">filip.melinscak@univie.ac.at)</a></span>

** Problem Setting
#+REVEAL_HTML: <div style="display: flex; flex-direction: column">
#+REVEAL_HTML: <div style="display: flex; flex-direction: row;">
#+REVEAL_HTML: <div style="width:80%">
*Machine Problem-Solving*
#+ATTR_REVEAL: :frag (t)
  - Increasingly taking over human domains
  - AI getting more complex \to black boxes \to lack of trust
  - Trust issues not new (Classical AI in 1950s) 
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="width:20%">
{{{revealimg("./img/dantzig_c.jpg.meta", "George Dantzig & Linear Programming (1947)", "40rh")}}}
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="display: flex; flex-direction: row;">
#+REVEAL_HTML: <div style="width:80%">
#+ATTR_REVEAL: :frag (t)
*Evaluating Human Interpretability*
#+ATTR_REVEAL: :frag (t)
  - Human-in-the-loop approach to evaluate interpretability
  - Understanding how a machine makes a decision
  - Critical for trust and collaboration with machines
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="width:20%">
#+ATTR_REVEAL: :frag (t)
{{{revealimg("./svg/human_loop.svg.meta", "Human-in-the-loop.", "40rh")}}}
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** Combinatorial Design Problems :noexport:
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;height: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:space-between;height:40%;">

{{{revealimg("./svg/knapsack.svg.meta","Knapsack Problem","30rh")}}}

{{{revealimg("./svg/minimum_spanning_tree.svg.meta","Minimum Spanning Tree","30rh")}}}

{{{revealimg("./svg/traveling_salesman.svg.meta","Traveling Salesman Problem","30rh")}}}


#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-height:60%">
#+ATTR_REVEAL: :frag (t)
  - Many real-world scenarios (logistics, etc.)
  - Can be solved by machines optimally (e.g., with Linear Programming)
  - Can be solved by humans (if problem is small enough)
#+ATTR_REVEAL: :frag (t)
  \to **Good setting for human-machine collaboration**
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
** Bin-Packing Problem
#+REVEAL_HTML: <div style="display:flex;flex-direction:row;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:70%;">
- Abstract representation of real-world scenarios (e.g., scheduling)
- Pack items into boxes
- Goal: Fill the boxes as much as possible
- Constraint: You cannot overfill the boxes
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-width:30%">
#+caption: A human performing the bin packing task.
 file:./img/binpacking.gif
 #+REVEAL_HTML: </div>
 #+REVEAL_HTML: </div>

** Optimal Solutions
#+ATTR_HTML: :height 600px :margin-top 0px :margin-bottom 0px
#+caption: The machine ([[https://developers.google.com/optimization/cp/cp_solver][CP-SAT]]) providing possible optimal solutions.
 file:./img/optimalsolutions.gif

# this variable defines how the figure is exported to html: oer-reveal--figure-div-template. It includes bare <p> tags which do not allow for further customization using css. we will add a class to it to make this possible.

** Question
*"What makes a solution interpretable?"*

** H1: Heuristic
- Humans use (greedy) heuristics \to greedy solution
- Similarity to greedy solution is measured by graph edit distance[cite/p:@sanfeliuDistanceMeasureAttributed1983] 
  
#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/heuristic.svg

#+ATTR_REVEAL: :frag (t)
*\to Solutions more interpretable if similar to greedy solution*
  
** H2: Simplicity

- Bins can look more or less simple/complex
- Formalized as log-probability that a mixture model (2 dirichlet, 1 geometric distribution) returns for each bin composition

#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/composition.svg

#+ATTR_REVEAL: :frag (t)
*\to Solutions more interpretable if simple*

** H3: Representation
 
- Items and boxes can be sorted by size or at random
- Formalized as rank correlation between the actual order and the sorted order

#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/order.svg

#+ATTR_REVEAL: :frag (t)
*\to Solutions more interpretable if sorted*
  
** Online-Experiment
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+REVEAL_HTML: <img src="svg/experiment_1.svg" alt="experiment overview" style="max-height:666px"/>
/N/ = 73 participants (pilot)

** Online-Experiment
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+REVEAL_HTML: <img src="svg/experiment_2.svg" alt="experiment detail" style="max-height:666px"/>
/N/ = 73 participants (pilot)

** Pilot Results
#+REVEAL_HTML: <div style="display:flex;flex-direction:row;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:60%;">

#+ATTR_HTML: :height 100% :margin-top 0px :margin-bottom 0px
#+caption:Fixed Effects Estimates of Predictor Variables on Choice in Multilevel Analysis. The plot displays the estimated fixed effects (with 95% confidence intervals) for the three predictors. The effects are adjusted for random effects at the group level, highlighting the marginal impact of each predictor on the outcome variable 'choice'.
 file:./img/results_choice_fixed_effects.png
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:40%;">
- *All three* predictors relevant for people's choices
- *Order* and *Heuristic* most influental
- *Explained variance*: /R²/\equal0.17
#+REVEAL_HTML: <div style="font-size:1.5rem;">
# Other findings:
#  - Considerable participant variability in all predictors
#  - Self-reported problem-solving skills ("PSI") and solving performance do not moderate choice
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>


** Pilot Results Machine Learning Analysis :noexport:

** Pilot Results Eye-tracking :noexport:
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;">
- *Gaze dwell times* predictive of choice (/r/ \equal 0.20)
- \Delta /t/ = /t_right_box/ - /t_left_box/
- Webcam-Eye-Tracking using [[https://webgazer.cs.brown.edu/][WebGazer.js]]
  
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:center;">


#+ATTR_HTML: :height 500px :margin-bottom 0px
#+caption:Heatmap indicating gaze dwell times during an evaluation trial.
file:./img/eye_example.png

# #+ATTR_HTML: :height 450px :margin-top 0px :margin-bottom 0px
# #+caption:Correlation (with 95% confidence interval) of gaze dwell time (right versus left) with the outcome variable 'choice', highlighting the relationship between where participants direct their gaze and their choices.
#  file:./img/eye_correlation.png

#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** Limitations
1. Did we measure *interpretability*?
2. Focus on *small problems* (diversity of solutions limited)
3. Only tested for optimal solutions, *no suboptimal solutions*

#+ATTR_REVEAL: :frag (t)
\to Follow-up publication, e.g., with collaboration task to address limitation 1
** Current Status

- *Completed*: Experimental design, analysis pipeline and pilot data collection

- *Pending*:

  1. Preregistration
  2. Confirmatory data collection & analysis
  3. Write draft
   
- *Publication*: Early 2025

- *Target journals*: /International Journal of Human-Computer Studies, IEEE Transactions on Cybernetics, IEEE Transactions on Systems, Man, and Cybernetics, Computers in Human Behavior/

** Possible future directions :noexport:
#+ATTR_HTML: :width 100% :margin-top 0px :margin-bottom 0px
file:./svg/next_steps.svg


** Takeaways
- Humans seem to use *solving heuristics during evaluation*​
- Adequate *visual representation* is requirement​
- All factors may play a bigger role in *larger problems*​
- *Validation* required


#+REVEAL_HTML: <div>
#+REVEAL_HTML: <div style="font-size:0.8rem;margin-top:2rem;margin-bottom:1rem;">This project is funded by <a href="https://www.ffg.at">www.ffg.at</a></div>
#+ATTR_HTML: :height 40px
file:./svg/ffg_logo_en.svg
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div>
#+REVEAL_HTML: <div style="font-size:0.8rem;margin-top:2rem;margin-bottom:1rem">and carried out in association with</div>
#+REVEAL_HTML: <div style="display:flex;justify-content:center;align-items:center;">

#+ATTR_HTML: :width 130px
file:./svg/UniWien_CMYK_A4.svg

#+ATTR_HTML: :height 50vpx
file:./svg/tu_logo.svg

#+ATTR_HTML: :width 120px
file:./img/tttech_logo.png

#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

* INTERACTION WITH EMOTIONAL AWARENESS

#+ATTR_HTML: :height 300px
file:./img/p2_crop.png
  
** Problem Setting
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;height: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:space-evenly;height:40%;">

#+ATTR_HTML: :height 250px
#+caption: The stimulus set.
file:./img/example_stimuli_large.png

#+ATTR_HTML: :height 250px
#+caption: Rating the fear level of each image.
file:./img/fear_ratings.png

#+ATTR_HTML: :height 250px
#+caption: Example rated images.
file:./img/example_stimuli_rated.png

#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-height:60%">

#+ATTR_REVEAL: :frag (t)
- *Interaction Setting*: Computerized Exposure Therapy
- *Images with emotional content* (e.g., spiders)
  - Usage requires information, e.g., how much fear they provoke
  - Fear ratings for 313 spider images [cite/p:@karnerSpiDaDatasetSelfreport2024]
#+ATTR_REVEAL: :frag (t)
- *Improvement*: Automatic evaluation of fear potential in new images

** Deep Neural Networks    
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

- Use deep neural networks to rate new images [cite/p:@lecunDeepLearning2015]
- Pre-trained on large datasets (ImageNet; [cite//bare:@dengImageNetLargescaleHierarchical2009])
{{{revealimg("./img/cnn_architecture.png.meta","Architecture of a convolutional neural network (CNN)","40rh")}}}

** Deep Neural Networks    
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

- Use deep neural networks to rate new images [cite/p:@lecunDeepLearning2015]
- Pre-trained on large datasets (ImageNet; [cite//bare:@dengImageNetLargescaleHierarchical2009])
{{{revealimg("./img/cnn_architecture_mod.png.meta","Architecture of a convolutional neural network (CNN)","40rh")}}}
- Transfer Learning [cite/p:@yosinskiHowTransferableAre2014]: Adapt & fine-tune on own data (313 images with fear ratings) \to *"Spider-Fear-Network"*

** Research questions

** Research questions

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;">
#+ATTR_REVEAL: :frag (t)
Q1: Can computer vision models built for object recognition learn a *latent construct* such as fear?
#+reveal_html:<div style="font-size:1.9rem;">
#+ATTR_REVEAL: :frag (t)
Q1-1: How much *data* do we need?
#+ATTR_REVEAL: :frag (t)
Q1-2: What *erros* will it make?
#+reveal_html:</div>
#+reveal_html:</div>

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;margin-top:3rem">
#+ATTR_REVEAL: :frag (t)
Q2: *How* does the model arrive at its judgments and how do they differ from *human judgments*?
#+reveal_html:</div>

** Methodology :noexport:
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;">
Q1: Can computer vision models built for object recognition learn a *latent construct* such as fear?
#+reveal_html:</div>


#+reveal_html:<div style="font-size:2.2rem;">
#+ATTR_REVEAL: :frag (t)
1. Find suitable deep learning *architecture* ([[https://timm.fast.ai][timm.fast.ai]])
2. Write *training* pipeline + train model
3. *Learning curve analysis*: Train multiple times with different amounts of data
4. *Error analysis*: Which images are difficult to predict
#+reveal_html:</div>

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;">
Q2: *How* does the model arrive at its judgments and how do they differ from *human judgments*?
#+reveal_html:</div>

#+reveal_html:<div style="font-size:2.2rem;">
#+ATTR_REVEAL: :frag (t)
5. *Explain predictions*: Highlight fear-relevant regions in each image using Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])
6. *Alignment analysis*: How do the model's judgments differ from human judgments (uncertainty, fear-relevant regions)
#+reveal_html:</div>



# prediction


** First Results

** Predictions


#+REVEAL_HTML: <div style="display:flex;flex-direction:row;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:60%;">
#+ATTR_HTML: :height 600px :margin-top 0px :margin-bottom 0px
#+caption:Predictive performance of the CNN model for each image.
file:./svg/cnn_results.svg
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:flex-start;width:40%;">
- Promising predictive accuracy
- Model: ResNet50 [cite/p:@heDeepResidualLearning2015]
- Training is possible on standard PC hardware (hours–days)
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>



# grad cam

** Attributions

** Attributions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])

  file:./img/Sp_283_gradcam.png

** Attributions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])

  file:./img/Sp_111_gradcam.png

** Attributions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])

  file:./img/Sp_285_gradcam.png

** Attributions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])

  file:./img/Sp_073_gradcam.png

** Feature Visualization
** Feature Visualization
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
What does a neuron in each layer look for?

#+ATTR_HTML: :height 70px
file:./svg/cnn_layers_early.svg

*Early Layers*

#+REVEAL_HTML: <div style="display:flex;flex-direction:row">
#+ATTR_HTML: :width 100%
#+CAPTION: Activation patterns in 9 example neurons for layer 1 (left), layer 2.1 (middle) and layer 2.2 (right).
  file:./img/features_early.png

** Feature Visualization
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
What does a neuron in each layer look for?

#+ATTR_HTML: :height 70px
file:./svg/cnn_layers_intermediate.svg

*Intermediate Layers*

#+REVEAL_HTML: <div style="display:flex;flex-direction:row">
#+ATTR_HTML: :width 100%
#+CAPTION: Activation patterns in example neurons for layer 3.1 (left), layer 3.2 (middle) and layer 3.3 (right).
  file:./img/features_intermediate.png
#+REVEAL_HTML: </div>

** Feature Visualization
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
What does a neuron in each layer look for?

#+ATTR_HTML: :height 70px
file:./svg/cnn_layers_last.svg

*Last Layers*

#+REVEAL_HTML: <div style="display:flex;flex-direction:row">
#+ATTR_HTML: :width 100%
#+CAPTION: Activation patterns in example neurons for layer 4.1 (left), layer 4.2 (middle) and layer 4.3 (right).
  file:./img/features_last.png
#+REVEAL_HTML: </div>

** Feature Visualization
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
What does a neuron in each layer look for?

#+ATTR_HTML: :height 70px
file:./svg/cnn_layers_final.svg

*Final Output Node*

#+ATTR_HTML: :width 100%
#+CAPTION: Three example images that maximize the final output "Fear".
  file:./img/features_final.png

** Why ... :noexport:

**... not just LLMs?**

1. CNNs *smaller* than transfomer LLMs (optimized for image tasks) and can run on affordable hardware (more efficient)
2. Most CNNs *free and open-source* (open science, reproducibility, ...)
3. Their decisions can be made *explainable* with methods like Grad-CAM (a consequence of 1 & 2)
   
** Current Status
- *Completed*: Concept, Analysis Pipeline
- *Pending*:
  - Explore more architectures
  - Error & learning curve analysis
  - Investigate overlap with human judgments
  - Write draft
- *Publication*: 2025
- *Target journals*: /International Journal of Human-Computer Studies, IEEE Transactions on Affective Computing, Computers in Human Behavior/


** Takeaways

- Computer vision models can learn latent construct like fear *(Q1)*
- Model’s judgments often, but not always, understandable (fear-eliciting stimulus not highlighted) *(Q2)*
  
* OPTIMAL INTERACTION IN EXPOSURE THERAPY

#+ATTR_HTML: :height 300px
file:./img/p3_other.png
#+REVEAL_HTML: <span style="font-size: 1.2rem">PI: Filip Melinscak (<a href="mailto:filip.melinscak@univie.ac.at">filip.melinscak@univie.ac.at)</a></span>


** Problem Setting

#+reveal_html: <div style="float:right;width:75%;">
#+ATTR_REVEAL: :frag (t)
- *Interaction Setting*: Computerized Exposure Therapy
- *Improvement*: Find optimal stimulus sequence based on patient's state
- *Challenges*:
  - Highly complex and individualized process
  - Inconsistent and subjective protocol tailoring
  - High-dimensional variable space
#+reveal_html: </div>

#+reveal_html: <div style="float:right;width:25%;">
#+ATTR_HTML: :height 300px :margin-top 0px :margin-bottom 0px
#+NAME: sequence_illustration
file:./img/p3_sequence.png
#+reveal_html: </div>

** Reinforcement Learning

#+caption: Exposure therapy as a reinforcement learning setting.
#+ATTR_HTML: :height 300px :margin-top 0px :margin-bottom 0px
#+NAME: aether_illustration
file:./img/aether_illustration.png

#+ATTR_REVEAL: :frag (t)
  - *Reinforcement Learning* (RL; [cite//bare:@suttonReinforcementLearningIntroduction2018]) offers a data-driven approach
  - Agent learns optimal actions through *trial and error*
  - *Therapist*: Deep RL Algorithms like Deep Q Networks (DQN; [cite//bare:@mnihPlayingAtariDeep2013])
  - *Simulated Patient*: e.g., [cite/text:@rescorla1972theory] $F_{\text{expected}} \leftarrow F_{\text{expected}} + \alpha (F_{\text{actual}} - F_{\text{expected}})$

# \[
# V_{\text{new}} = V_{\text{old}} + \alpha (\lambda - V_{\text{old}})
# \]


** Research Questions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;">
#+ATTR_REVEAL: :frag (t)
Q1: How can RL *model fear extinction* and optimize therapy protocols?
#+reveal_html:</div>

#+reveal_html:<div style="border-radius:12px;background-color:lightblue;margin-top:3rem">
#+ATTR_REVEAL: :frag (t)
Q2: Can RL improve *consistency and objectivity* in exposure therapy?
#+reveal_html:</div>
    
** Planned Methodology
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+reveal_html:<div style="font-size:2.6rem;border-radius:12px;background-color:lightblue;">
Q1: How can RL *model fear extinction* and optimize therapy protocols?
#+reveal_html:</div>

#+reveal_html:<div style="font-size:2.4rem;">
#+ATTR_REVEAL: :frag (t)
    1. Selecting *RL algorithms* based on theoretical and practical applicability
    2. Conducting preliminary *simulations* using models such as Rescorla-Wagner
    3. Defining and testing *reward functions* to guide learning
    4. Running iterative computational *experiments* to refine algorithms
#+reveal_html:</div>

#+reveal_html:<div style="font-size:2.6rem;border-radius:12px;background-color:lightblue;">
Q2: Can RL improve *consistency and objectivity* in exposure therapy?
#+reveal_html:</div>

#+reveal_html:<div style="font-size:2.4rem;">
#+ATTR_REVEAL: :frag (t)
Empirical validation with real subjects
#+reveal_html:</div>

** Current Status

- *Completed*: Concept (partly)

- *Pending*:

  1. Literature Review
  2. Select Algorithms
  3. Run Simulations
  4. Design Experiment
   
- *Publication*: 2026

- *Target journals*: /Foundations and Trends in Machine Learning, International Journal of Human-Computer Studies, IEEE Transactions on Affective Computing, Computers in Human Behavior/
  
** Takeaways :noexport:
    - Aim to advance computer-aided exposure therapy
    - Find good exposure therapy protocols
    - Address limitations through RL
    - Potential to improve therapy outcomes
      
* Open Science

#+ATTR_HTML: :height 300px
[[file:./img/open_science.jpg]]

All data, code, material, preregistrations will be made openly available on [[https://osf.io/][osf.io]]

* Schedule


#+reveal_html:<div style="float:left;width:7%;display:flex;flex-direction:column;justify-content:space-evenly;height:600px">
#+attr_html: :max-width 100px
file:./img/p1_update.png
#+attr_html: :max-width 100px
file:./img/p2_crop.png
#+attr_html: :max-width 100px
file:./img/p3_other.png
#+reveal_html:</div>

#+reveal_html:<div style="float:right;width:93%;justify-items:flex-start">
#+ATTR_HTML: :width 1350px :margin-top 0px
 file:./svg/gantt.svg
#+reveal_html:</div>

* Summary
# blue box

#+reveal_html:<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;"><b>Enhancing Human-Computer Interaction (HCI)</b></div>
#+ATTR_REVEAL: :frag (t)
1. First results show us how we can enhance *trust* by finding factors that matter for interpretability
2. AI models can learn *emotional potential in images*, but the "how" remains open
3. Reinforcement learning is a promising approach for finding good *interaction protocols* in computerized exposure therapy
* Source code :noexport:
#+begin_src python -n :results output
import numpy as np

np.random.seed(12)
x = np.random.randint(100)
print(x)
#+end_src

#+RESULTS:
: 75

* Equations :noexport:
  - Here is an inline equation: \( E = mc^2 \).
  - Here is a displayed equation:
    \[
    a^2 + b^2 = c^2
    \]
* References
   :PROPERTIES:
   :CUSTOM_ID: bibliography
   :END:


# adjust font-size and line-width and in css/custom.css if you cannot put all references on 1 slide. a better solution that allows splitting the bibliography across slides still needs to be found. 

# note: this uses apa.csl which is downloaded from the zotero style repository and makes sure that the bibliography is formatted correctly. https://www.zotero.org/styles

#+print_bibliography:
