:PROPERTIES:
:ID:       dae7ee8b-4424-404a-be4c-df415e5abab7
:END:
#+title: Machine Learning for Human-Centered Solutions
#+subtitle: Interpretability, Emotion Recognition, and Therapeutic Innovation
#+project: Faculty Open Presentation 2024
#+created: [2024-09-26 Thu]
#+last_modified: [2024-09-26 Thu 21:21]
#+author: Dominik Pegler
#+date: 2024-09-16
#+REVEAL_THEME: white
#+REVEAL_MARGIN: 0.1
#+REVEAL_TRANS: slide
#+REVEAL_SPEED: fast
# +reveal_slide_footer: <div>Footer</div>
#+reveal_single_file: t
#+OPTIONS: num:nil toc:nil reveal_progress:t reveal_control:t reveal_slide_number:t 
#+OPTIONS: reveal_width:1200 reveal_height:800 reveal_center:t reveal_keyboard:t reveal_overview:t
#+BIBLIOGRAPHY: /home/user/Dropbox/org/ref/ref.bib
#+cite_export: csl apa.csl
#+REVEAL_EXTRA_CSS: css/custom.css
#+REVEAL_TITLE_SLIDE:<div><h1>%t</h1><h3 style="color:#6b6b6b">%s<h3><p style="text-transform:none;color:black;font-weight:normal">%a<p></div>
#+MACRO: revealimg (eval (oer-reveal-export-attribution $1 $2 $3 $4 $5 $6))
#+MACRO: reveallicense (eval (oer-reveal-export-attribution $1 nil $2 $3 $4 $5 $6))

* reveal.js infos :noexport:

- https://earvingad.github.io/posts/img/orgreveal/orgreveal.html
- https://github.com/emacsmirror/org-re-reveal
- https://revealjs.com/config/
- https://ertwro.github.io/githubppt/Readmeofficial.html

on how to create reusable css classes
- https://www.gibiris.org/eo-blog/posts/2022/09/28_org-reveal-and-gridded-layouts.html

- TODO: check how to insert image licenses using templates https://oer.gitlab.io/emacs-reveal-howto/howto.html#/slide-figure-with-meta-data
- TODO: how to create simple diagrams with diagram+d3js plugins
- TODO: split bibliography if it gets too long
- TODO: find simpler way to create (css classes?) to particular slide layouts

* Overview


# blue box

#+reveal_html:<div style="background:lightblue;border-radius:20px;color:#2e2e2e;padding:4px;">Enhancing cognitive and affective processes in human-AI interaction through machine learning</div>

# outer div

#+reveal_html:<div style="display:flex;flex-direction:column;font-size:2.0rem">

# grey box 1

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:8px;margin-bottom:4px;padding:8px">

# header 1

#+reveal_html:<div style="color:#7e7e7e;font-weight:bold;font-size:1.4rem;margin-bottom:4px">Cognitive Domain: Problem Solving</div>

# item 1-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p1.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">#1 – Interpretability</div>
#+reveal_html:<div style="font-size:1.6rem;text-align:left">Enhancing joint human-machine problem solving by optimizing the machine for interpretability</div>

# end of item 1-1
#+reveal_html:</div>
#+reveal_html:</div>

# end of grey box 1
#+reveal_html:</div>

# grey box 2

#+reveal_html:<div style="display:flex;flex-direction:column;align-items:flex-start;background:#efefef;border-radius:20px;margin-top:4px;margin-bottom:0px;padding:8px">

# header 2

#+reveal_html:<div style="color:#7e7e7e;font-size:1.4rem;font-weight:bold;margin-bottom:4px">Affective Domain: Exposure Therapy</div>

# item 2-1

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p2.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">#2 – Fear Prediction in Images</div>


#+reveal_html:<div style="font-size:1.6rem;text-align:left">Advancing computer-aided Exposure Therapy by automatically evaluating fear-related stimuli through artificial neural networks.</div>

# end of item 2-1
#+reveal_html:</div>
#+reveal_html:</div>

# item 2-2

#+reveal_html:<div style="display:flex; flex-direction:row; margin-top:1rem">

#+reveal_html:<div style="width:15%;">
#+ATTR_HTML: :height 100px
file:./img/p3.png
#+reveal_html:</div>

#+reveal_html:<div style="display:flex; flex-direction:column;align-items:flex-start;margin-left:2rem;width:85%;">

#+reveal_html:<div style="font-weight:bold;">#3 – Optimizing Stimulus Sequences</div>

#+reveal_html:<div style="font-size:1.6rem;text-align:left"> Advancing computer-aided Exposure Therapy by optimizing therapy protocols using reinforcement learning (RL).</div>

# end of item 2-2
#+reveal_html:</div>
#+reveal_html:</div>


# end of grey box 2
#+reveal_html:</div>

# end of outer div
#+reveal_html:</div>


* #1

*INTERPRETABILITY OF MACHINE-GENERATED SOLUTIONS TO COMBINATORIAL DESIGN PROBLEMS*

#+ATTR_HTML: :height 490px
file:./img/p1.png


** Problem Setting

#+begin_notes
Today, I am going to present you a new study on the topic of human-machine colaboration.
#+end_notes

#+REVEAL_HTML: <div style="display: flex; flex-direction: column">
#+REVEAL_HTML: <div style="display: flex; flex-direction: row;">
#+REVEAL_HTML: <div style="width:80%">
*Machine Problem-Solving*
  - Increasingly taking over human domains
  - AI getting more complex \to black boxes \to lack of trust
  - Trust issues not new (Classical AI in 1950s) 
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="width:20%">
{{{revealimg("./img/dantzig.jpg.meta", "George Dantzig \(1914-2005\)\, father of linear programming", "22rh")}}}
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+ATTR_REVEAL: :frag (t)
#+REVEAL_HTML: <div style="display: flex; flex-direction: row;">
#+REVEAL_HTML: <div style="width:80%">
*Evaluating Human Interpretability*
  - Human-in-the-loop approach to evaluate interpretability
  - Understanding how a machine makes a decision
  - Critical for trust and collaboration with machines
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="width:20%">
{{{revealimg("./svg/human_loop.svg.meta", "Human-in-the-loop: Enhanced algorithms through continuous human input.", "22rh")}}}
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** Combinatorial Design Problems
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;height: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:space-between;height:40%;">

{{{revealimg("./svg/knapsack.svg.meta","Knapsack Problem","22rh")}}}

{{{revealimg("./svg/minimum_spanning_tree.svg.meta","Minimum Spanning Tree","22rh")}}}

{{{revealimg("./svg/traveling_salesman.svg.meta","Traveling Salesman Problem","22rh")}}}


#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-height:60%">
#+ATTR_REVEAL: :frag (t)
  - Many real-world scenarios (logistics, etc.)
  - Can be solved by machines optimally (e.g., with Linear Programming)
  - Can be solved by humans (if problem is small enough)
#+ATTR_REVEAL: :frag (t)
  \to **Good setting for human-machine collaboration**
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
** Bin-Packing Problem

#+REVEAL_HTML: <div style="display:flex;flex-direction:row;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:70%;">
- Abstract representation of real-world scenarios (e.g., scheduling)
- Pack items into boxes
- Goal: Fill the boxes as much as possible
- Constraint: You cannot overfill the boxes
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-width:30%">
#+caption: A human performing the bin packing task.
 file:./img/binpacking.gif
 #+REVEAL_HTML: </div>
 #+REVEAL_HTML: </div>

** Optimal Solutions
#+ATTR_HTML: :height 600px :margin-top 0px :margin-bottom 0px
#+caption: The machine ([[https://developers.google.com/optimization/cp/cp_solver][CP-SAT]]) providing possible optimal solutions.
 file:./img/optimalsolutions.gif

# this variable defines how the figure is exported to html: oer-reveal--figure-div-template. It includes bare <p> tags which do not allow for further customization using css. we will add a class to it to make this possible.

** Question
*"What makes a solution interpretable?"*

** H1: Heuristic
- Humans use (greedy) heuristics \to greedy solution
- Similarity to greedy solution is measured by graph edit distance[cite/p:@sanfeliuDistanceMeasureAttributed1983] 
  
#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/heuristic.svg

*\to Solutions more interpretable if similar to greedy solution*
  
** H2: Simplicity

- Bins can look more or less simple/complex
- Formalized as log-probability that a mixture model (2 dirichlet, 1 geometric distribution) returns for each bin composition

#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/composition.svg

*\to Solutions more interpretable if simple*

** H3: Representation
 
- Items and boxes can be sorted by size or at random
- Formalized as rank correlation between the actual order and the sorted order

#+ATTR_HTML: :height 480px :margin-top 0px :margin-bottom 0px
 file:./svg/order.svg

*\to Solutions more interpretable if sorted*
  
** Online-Experiment
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+REVEAL_HTML: <img src="svg/experiment_1.svg" alt="experiment overview"/>
/N/ = 73 participants (pilot)

** Online-Experiment
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:

#+REVEAL_HTML: <img src="svg/experiment_2.svg" alt="experiment overview"/>
/N/ = 73 participants (pilot)

** Results Multilevel Analysis
#+REVEAL_HTML: <div style="display:flex;flex-direction:row;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:60%;">

#+ATTR_HTML: :height 100% :margin-top 0px :margin-bottom 0px
#+caption:Fixed Effects Estimates of Predictor Variables on Choice in Multilevel Analysis. The plot displays the estimated fixed effects (with 95% confidence intervals) for the three predictors. The effects are adjusted for random effects at the group level, highlighting the marginal impact of each predictor on the outcome variable 'choice'.
 file:./img/results_choice_fixed_effects.png
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;width:40%;">
- *All three* predictors relevant for people's choices
- *Order* and *Heuristic* most influental
- *Moderate effect*: /R²/\equal0.17 [cite/p:@cohenStatisticalPowerAnalysis1988]
#+REVEAL_HTML: <div style="font-size:1.5rem;">
Other findings:
  - Considerable participant variability in all predictors
  - Self-reported problem-solving skills ("PSI") and solving performance do not moderate choice
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>


** Results Machine Learning Analysis :noexport:

** Results Eye-tracking Analysis
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;width: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:column;justify-content:center;">
- *Gaze dwell times* predictive of choice (/r/ \equal 0.20)
- \Delta /t/ = /t_right_box/ - /t_left_box/
- Webcam-Eye-Tracking using [[https://webgazer.cs.brown.edu/][WebGazer.js]]
  
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:center;">


#+ATTR_HTML: :margin-top 0px :margin-bottom 0px
#+caption:Heatmap indicating gaze dwell times during an evaluation trial.
file:./img/eye_example.png

#+ATTR_HTML: :height 450px :margin-top 0px :margin-bottom 0px
#+caption:Correlation (with 95% confidence interval) of gaze dwell time (right versus left) with the outcome variable 'choice', highlighting the relationship between where participants direct their gaze and their choices.
 file:./img/eye_correlation.png

#+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** Limitations
- Did we measure *interpretability*?
- Focus on *small problems* (diversity of solutions limited)
- Only tested for optimal solutions, *no suboptimal solutions*
- Eye-Tracking via webcam very noisy

# maybe remove participants with very low accuracy? is accuracy stored somewhere?

*Possible next steps*
#+ATTR_HTML: :width 100% :margin-top 0px :margin-bottom 0px
 file:./svg/next_steps.svg

** Limitations :noexport:
- Did we measure *interpretability*?
- Focus on *small problems* (diversity of solutions limited)
- Only tested for optimal solutions, *no suboptimal solutions*

*Possible next steps*

1. Collaboration task (to validate results)
2. One cognitive model instead of three
3. Neural network analysis (CNNs, GNNs) to learn participants' preferences and try to explain them

** Takeaways
- Humans seem to use *solving heuristics during evaluation*​
- Adequate *visual representation* is requirement​
- All factors may play a bigger role in *larger problems*​
- *Validation* required

#+REVEAL_HTML: <div style="font-size:0.8rem;margin-top:5rem">
This project is supported by [[https://www.ffg.at][www.ffg.at]]
#+REVEAL_HTML: </div>
#+ATTR_HTML: :height 40px
file:./svg/ffg_logo_en.svg

* #2

  *LEARNING AND LOCALIZING FEAR WITH COMPUTER VISION MODELS*

#+ATTR_HTML: :height 400px
file:./img/p2_alt.png
  
** Problem Setting
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
#+reveal_html: <div style="float:left;width:78%;">
*Background: "Affective Computing" [cite/p:@picardAffectiveComputing1997]*
  - Technology that relates to, arises from, or influences emotions
  - For effective and natural human-computer interactions, computers must not only recognize but also respond to human emotions

*Our study: Phobia Research*
- Aim: Advance computer-aided exposure therapy
- Focus: Spider phobia
#+reveal_html: </div>

#+reveal_html: <div style="float:right;width:22%;">
{{{revealimg("./img/affective_computing.jpg.meta", "Affective Computing (1997) by Rosalind Picard.", 45rh")}}}
# {{{revealimg("./img/picard.jpg.meta", "Scientist\, inventor\, entrepreneur\, author\, and engineer Rosalind Picard.", "30rh")}}}
#+reveal_html: </div>
** Problem Setting
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:


#+REVEAL_HTML: <div style="display:flex;flex-direction:column;height: 100%">
#+REVEAL_HTML: <div style="display:flex; flex-direction:row;justify-content:space-evenly;height:40%;">


#+ATTR_HTML: :height 250px
#+caption: The stimulus set.
file:./img/example_stimuli_large.png

#+ATTR_HTML: :height 250px
#+caption: Rating the fear level of each image.
file:./img/fear_ratings.png

#+ATTR_HTML: :height 250px
#+caption: Example rated images.
file:./img/example_stimuli_rated.png

#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div style="display:flex;flex-direction:column;max-height:60%">

- *Spider images* for exposure therapy
  - Usage requires information, e.g., how much fear they provoke
  - Fear ratings for 313 spider images collected
#+ATTR_REVEAL: :frag (t)
- *Problem:* Set is limited to 313
  - Constantly collecting new fear ratings for each new stimulus not
    feasible
    

** Deep Neural Networks    
- Solution: Use deep neural networks to create larger stimulus sets [cite/p:@lecunDeepLearning2015]
- Pre-trained on large datasets [cite/p:@dengImageNetLargescaleHierarchical2009]
- Transfer Learning [cite/p:@yosinskiHowTransferableAre2014]: Fine-tune on own data (313 images with fear ratings)
- Provide a fear rating for any new image


{{{revealimg("./img/cnn_architecture.png.meta","Architecture of a convolutional neural network (CNN)","40rh")}}}

** Research questions

#+ATTR_REVEAL: :frag (t)
Q1: Can a computer vision model built for object recognition learn a *latent construct* (an affective response) such as fear? If yes, 

#+ATTR_REVEAL: :frag (t)
   A) How much *data* do we need?
   B) What *erros* will it make?

#+ATTR_REVEAL: :frag (t)
Q2: *How* does the model arrive at its judgments and how do they differ from *human judgments*?

** Methodology
#+ATTR_REVEAL: :frag (t)

*Q1: Can it learn fear?*

1. Find suitable deep learning *architecture* ([[https://timm.fast.ai][timm.fast.ai]])
2. Write *training* pipeline + train model
3. *Learning curve analysis*: Train multiple times with different amounts of data
4. *Error analysis*: Which images are difficult to predict

*Q2: How does it manifest?*

5. *Explain predictions*: Highlight fear-relevant regions in each image using Gradient-weighted Class Activation Mapping (Grad-CAM; [cite//bare:@selvarajuGradCAMVisualExplanations2020])
6. *Alignment analysis*: How do the model's judgments differ from human judgments (uncertainty, fear-relevant regions)

# 6. beyond accuracy

** Preliminary Results: Predictions
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Predictions with ResNet50 [cite/p:@heDeepResidualLearning2015]
- Some info about training time

  # (put bullets to the side)
  
#+ATTR_HTML: :height 600px :margin-top 0px :margin-bottom 0px
 file:./svg/cnn_results.svg

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_046_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_111_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

 file:./img/Sp_012_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_092_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_283_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_098_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_078_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_073_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_285_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Preliminary Results: Explanations
:PROPERTIES:
:REVEAL_EXTRA_ATTR: data-auto-animate
:END:
- Explanations with Grad-CAM [cite/p:@selvarajuGradCAMVisualExplanations2020]

  file:./img/Sp_085_gradcam.png

# Sp_012_gradcam.png
# Sp_046_gradcam.png
# Sp_073_gradcam.png
# Sp_078_gradcam.png
# Sp_085_gradcam.png
# Sp_092_gradcam.png
# Sp_098_gradcam.png
# Sp_111_gradcam.png
# Sp_283_gradcam.png
# Sp_285_gradcam.png

** Why ...

**... not just LLMs?**

1. Most CNNs *free and open-source* (open science, reproducibility, ...)
2. CNNs *smaller* than transfomer LLMs (good at one task) and can run on affordable hardware
3. Their decisions can be made *explainable* with methods like Grad-CAM (a consequence of 1 & 2)

* #3

*LEARNING OPTIMAL EXPOSURE THERAPY PROTOCOLS WITH REINFORCEMENT LEARNING*

#+ATTR_HTML: :height 400px
file:./img/p3.png

#+REVEAL_HTML: <span style="font-size: 1.33rem">PI: Filip Melinscak</span>

** Problem Setting

- ...
- ...
- Reinforcement Learning (RL; [cite//bare:@suttonReinforcementLearningIntroduction2018])
* Schedule
 file:./svg/gantt.svg
* Summary
- ...
- ...
- ...
* Source code :noexport:
#+begin_src python -n :results output
import numpy as np

np.random.seed(12)
x = np.random.randint(100)
print(x)
#+end_src

#+RESULTS:
: 75

* Equations :noexport:
  - Here is an inline equation: \( E = mc^2 \).
  - Here is a displayed equation:
    \[
    a^2 + b^2 = c^2
    \]
* References
   :PROPERTIES:
   :CUSTOM_ID: bibliography
   :END:

# adjust font-size and line-width and in css/custom.css if you cannot put all references on 1 slide. a better solution that allows splitting the bibliography across slides still needs to be found. 

# note: this uses apa.csl which is downloaded from the zotero style repository and makes sure that the bibliography is formatted correctly. https://www.zotero.org/styles

#+print_bibliography:
